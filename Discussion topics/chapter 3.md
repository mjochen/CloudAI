# Discussion topics chapter 3 - Models Quality

1. What is the difference between classification and regression models?
1. Can you give examples of real-world problems that would use classification vs regression?
1. What kind of questions or predictions are best suited for regression models?
1. What are the main types of machine learning? How do they differ?
1. Why do we split the dataset into training and test sets? How does this help in evaluating the model's performance?
1. Why do we split data into training, validation, and test sets?
1. What could go wrong if you don’t properly randomize your data before splitting?
1. What is the role of the validation set when tuning model parameters?
1. How can the sensitivity to the train/test split affect model performance? What strategies can you use to mitigate this sensitivity?
1. Why is it important for a model to generalize to unseen data? What are the risks of a model that doesn't generalize well?
1. What are the signs of overfitting and underfitting in a model? How can you address these issues?
1. What is the bias-variance tradeoff? How do you find the right balance between bias and variance in a model?
1. How do you model more complex patterns in data? What techniques can be used to capture these patterns effectively?
1. Why is it important to start with data exploration before modeling?
1. How do you determine if a model is good? What metrics and methods can you use to evaluate its performance?
1. What does RMSE (Root Mean Square Error) tell us about a model's performance? Why is it important to compare RMSE on both training and test datasets?
1. What does RMSE measure and why is it useful?
1. How does RMSE differ from MAE?
1. What does R² tell us about a regression model?
1. Why do classification metrics not apply to regression problems?
1. What are TP, FP, FN, and TN in a confusion matrix?
1. Give some examples on why you'd want to optimize a model towards more TP (and more FP) or more TN (and more FN).
1. What does sensitivity (recall) measure, and why is it important in medical diagnostics?
1. What does specificity measure, and when is it more important than sensitivity?
1. Can you think of a scenario where high sensitivity is preferred over high specificity, and vice versa?
1. What does the F1-score balance, and when is it most useful?
1. What challenges arise when evaluating multiclass classification models?
1. What parameters can be calculated to assess the quality of multiclass classification models?
1. What is the difference between macro and micro averaging?
1. In multiclass classification, what are macro and micro average? What are the advantages of both?
1. When would you use macro average over micro average?
1. What does changing the classification threshold do to sensitivity and specificity?
1. What is an ROC and the AUC?
1. What does the ROC curve represent?
1. Why is the diagonal line in an ROC curve considered a “random classifier”?
1. How can ROC curves help compare different models?
1. What does AUC-ROC measure and why is it useful?
1. Why might ROC curves be misleading in imbalanced datasets?

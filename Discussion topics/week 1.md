# Discussion topics week 1

## General statistics:
1. The chances of somebody dying on their birthday are 13% higher than on any other day. Is this possible? Explain.
1. What does the following graph say about the test of which it represents the scores?

    ![](files/2023-04-11-18-33-40.png)

1. The following graph tries to make the point that global warming is out of control. But what is wrong with it?

    ![](files/2023-04-11-18-33-17.png)

1. Under which conditions can a pie chart be used?
1. Explain the difference between “variation” and “covariation” with an example of a hypothetical student-grade-dataset.
1. What is the difference between a bar plot and a histogram? When do we use them?
1. How can you explore covariation? Are there different graphs that we can use?
1. Explain covariance and correlation with an example.

## Ethics in AI

1. Suppose we record all information on our students (gender, race, height, shoe size, schooling history, profession of parents, …) and use this to only allow students with a reasonable chance of succeeding to start in the first year. Would that be a good idea? Would it be an ethical idea?
(Good: something that works, that does what it is supposed to, Ethical: Being in accordance with the accepted principles of right and wrong that govern the conduct of a profession. E.g., robbing a bank is a good idea because it gets you a lot of money fast, but it isn’t an ethical idea because it is illegal and people may die.)
1. Suppose we record all school-related information on our students (class attendance, exam results, how long they studied, …) and compare this with the results of students finishing the first year, it would give us an insight in their chances of finishing our program in a timely fashion.
What if we forbid students that we think would not finish or take more than 4 years to continue in our program. Would that be a good/ethical idea?
What if we wouldn’t forbid them, but simply gave them a very strong warning (unless you dramatically change some of these parameters, you won’t be finishing this program according to the data). Would that be a good/ethical idea?

## Our First model

1. "The model should generalize to unseen data if you are using it to predict. If it’s just to explain the data overfitting isn’t a problem." Explain!
1. What are the key differences between classification and regression in supervised learning? Can you provide examples of each?
1. How does the concept of multidimensional space help in understanding the results of unsupervised learning algorithms?
1. Why is feature engineering crucial in predictive modeling? Can you give an example of a feature that significantly improved a model's performance?
1. What are the differences between linear and polynomial regression? When might you choose one over the other?
1. Why is RMSE a preferred metric for evaluating regression models? What are its advantages and disadvantages compared to other metrics?
1. What can residuals tell us about the performance of a model? How can you use residuals to improve your model?

## Model Quality

1. How do you determine if a model is good? What metrics and methods can you use to evaluate its performance?
1. Why is it important for a model to generalize to unseen data? What are the risks of a model that doesn't generalize well?
1. Why do we split the dataset into training and test sets? How does this help in evaluating the model's performance?
1. What does RMSE (Root Mean Square Error) tell us about a model's performance? Why is it important to compare RMSE on both training and test datasets?
1. What are the signs of overfitting and underfitting in a model? How can you address these issues?
1. How does polynomial regression differ from linear regression? What are the challenges associated with using higher-order polynomials?
1. What is the bias-variance tradeoff? How do you find the right balance between bias and variance in a model?
1. What parameters can be calculated to assess the quality of multiclass classification models?
1. In multiclass classification, what are macro and micro average? What are the advantages of both?
1. How can the sensitivity to the train/test split affect model performance? What strategies can you use to mitigate this sensitivity?
1. How do you model more complex patterns in data? What techniques can be used to capture these patterns effectively?
1. What common issues can arise in the data you receive? How can you address problems like outliers, missing values, and bias?
1. How do outliers impact your data analysis and model performance? What are some strategies for handling outliers?
1. Why doesn't the Z-score matter in a column that is used in one hot encoding?
1. What is an ROC and the AUC?
1. What are the differences between nominal and ordinal data? How do you handle categorical data in machine learning models?
1. What are the different strategies for handling missing values in a dataset? When might you choose one method over another?
1. What are the different types of bias that can affect your data and models? How can you mitigate these biases?
1. Why is scaling important in data preprocessing? What are the differences between min-max scaling and standardization?
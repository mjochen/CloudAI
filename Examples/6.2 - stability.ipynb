{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability\n",
    "\n",
    "## The AWS example\n",
    "\n",
    "On the AWS-course, module 4, section 2, part 2 it is said that two random list of numbers won't have a good correlation, but if they share a similar slope they will. Lets experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "\n",
    "random.seed(5)\n",
    "\n",
    "l1 = [random.randint(1, 10) for i in range(100)]\n",
    "l2 = [random.randint(1, 10) for i in range(100)]\n",
    "\n",
    "numpy.corrcoef(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What doe the series look like in a graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "\n",
    "plt.scatter(y = l1, x=range(100))\n",
    "plt.scatter(y = l2, x=range(100))\n",
    "\n",
    "plt.legend([\"L1\", \"L2\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation of 1.3% is low indeed. But what if whe introduce a slope of 10%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [(i/10) + random.randint(1, 10) for i in range(100)]\n",
    "l2 = [(i/10) + random.randint(1, 10) for i in range(100)]\n",
    "\n",
    "numpy.corrcoef(l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "\n",
    "plt.scatter(y = l1, x=range(100))\n",
    "plt.scatter(y = l2, x=range(100))\n",
    "\n",
    "plt.legend([\"L1\", \"L2\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a smaller slope of 1%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [(i/100) + random.randint(1, 10) for i in range(100)]\n",
    "l2 = [(i/100) + random.randint(1, 10) for i in range(100)]\n",
    "\n",
    "print(numpy.corrcoef(l1, l2))\n",
    "\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "\n",
    "plt.scatter(y = l1, x=range(100))\n",
    "plt.scatter(y = l2, x=range(100))\n",
    "\n",
    "plt.legend([\"L1\", \"L2\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much smaller, but still significant. What about a 100% slope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [(i) + random.randint(1, 10) for i in range(100)]\n",
    "l2 = [(i) + random.randint(1, 10) for i in range(100)]\n",
    "\n",
    "print(numpy.corrcoef(l1, l2))\n",
    "\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "\n",
    "plt.scatter(y = l1, x=range(100))\n",
    "plt.scatter(y = l2, x=range(100))\n",
    "\n",
    "plt.legend([\"L1\", \"L2\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is nearly 1. Could we graph this out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_j = []\n",
    "data_corr = []\n",
    "\n",
    "for j in range(1,101):\n",
    "    l1 = [(i/j) + random.randint(1, 10) for i in range(100)]\n",
    "    l2 = [(i/j) + random.randint(1, 10) for i in range(100)]\n",
    "\n",
    "    data_j.append(j)\n",
    "    data_corr.append(numpy.corrcoef(l1, l2)[0][1])\n",
    "    \n",
    "# print(data)\n",
    "\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "\n",
    "plt.scatter(y = data_j, x=data_corr)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is that:\n",
    "\n",
    "* At low levels of J, so the high slope-value, the correlation is really good.\n",
    "* As J gets above 20, so a slope 5% of the correlation becomes quite low.\n",
    "* Sometimes, starting at a slope of about 1/40 (2.5%) the correlation is even negative, meaning the random data completely counteracted the slope we introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The random youtuber explaining Stationarity\n",
    "\n",
    "[This one](https://www.youtube.com/watch?v=wNfXI81pSsM)\n",
    "\n",
    "We are using the following data series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([300, 310, 420, 530, 600], columns = ['values'])\n",
    "df_rolling = df.rolling(window=2).mean()\n",
    "\n",
    "ax = df.plot()\n",
    "df_rolling.plot(ax=ax)\n",
    "ax.legend([\"Original\", \"Rolling\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although what he is saying makes sense, the data does not. There's simply not enough data to show why smoothing is neccessary.\n",
    "\n",
    "Let's look at [another random youtuber](https://www.youtube.com/watch?v=621MSxpYv60). He uses [this](https://www.kaggle.com/datasets/ashfakyeafi/air-passenger-data-for-time-series-analysis/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air = pd.read_csv(\"files/AirPassengers.csv\")\n",
    "df_air.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is that not only is the data going up, but the fluctuations are also increasing. This means it is not stationary because:\n",
    "\n",
    "- There is a clear increasing trend\n",
    "- There is a clear increasing variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air[\"Passenger_Diff\"] = df_air[\"#Passengers\"].diff()\n",
    "df_air[\"Passenger_Diff\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completely removes the trend. That means the mean is now constant. The variance however is still unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_air[\"Passenger_Log\"] = np.log(df_air[\"#Passengers\"])\n",
    "df_air[\"Passenger_Log\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the trend is back, but the variance is constant. This means that the variance was actually exponential in our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air[\"Passenger_Diff_Log\"] = df_air[\"Passenger_Log\"].diff()\n",
    "df_air[\"Passenger_Diff_Log\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the video a statistical test is now used to confirm that our data is stationary, but that would take us to far. The end of the video is though.\n",
    "\n",
    "Why are we doing this? Aren't we deleting interesting information from the data?\n",
    "\n",
    "No.\n",
    "\n",
    "Because every data point has it's own mean and variance, it belongs to a different distribution. This was not the case when predicting non-time-series data, for example: when predicting ice cream sales as a function of temperature (warm: more ice creams sold) both the temperatures and the ice creams have a mean. These averages never change! When we look at ice cream sales as a function of time (ice cream sales going up from January until July) there is no common mean anymore, and therefore they don't have a common distribution.\n",
    "\n",
    "The good news is that most models (like Arima) fix this for us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model quality\n",
    "\n",
    "__Disclaimer:__ this is a markdown file with runnable code, which why it's a Jupyter notebook. But don't treat it as you normally would a notebook (scan the text diagonally, run the code, repeat) but as you would a markdown-file containing very important text.\n",
    "\n",
    "__Disclaimer 2:__ This explanation uses a very bad \"is it a cat or not?\"-model as example. You should ignore 2 things about this model:\n",
    "\n",
    "1. Cat or not cat is a deep learning model, where we are working with machine learning. The results of a machine learning model are the same though, but less graphical (a fraudulent transaction is nowhere near as cute as a cat).\n",
    "2. The model is awful. Any script-kiddy copying from youtube can make a better cat-dog model. The reason we work with such a bad model is because that makes it easier to align the results with the model. (0,9987 is less than 0,99978, but it easier to see the difference between 0,18 and 0,64)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, test and validation\n",
    "\n",
    "You split your data in three parts. The exact percentages differ, but training is the biggest (70%-80%), and the remainder is divided equally between validation and test.\n",
    "\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets) explains this very well:\n",
    "\n",
    "__Training set:__\n",
    "\n",
    "A training data set is a data set of examples used during the learning process and is used to fit the parameters (e.g., weights) of, for example, a classifier.\n",
    "\n",
    "For classification tasks, a supervised learning algorithm looks at the training data set to determine, or learn, the optimal combinations of variables that will generate a good predictive model. The goal is to produce a trained (fitted) model that generalizes well to new, unknown data. The fitted model is evaluated using \"new\" examples from the held-out datasets (validation and test datasets) to estimate the model's accuracy in classifying new data.\n",
    "\n",
    "__Test data set:__\n",
    "\n",
    "A test data set is a data set that is independent of the training data set, but that follows the same probability distribution as the training data set. If a model fit to the training data set also fits the test data set well, minimal overfitting has taken place (see figure below). A better fitting of the training data set as opposed to the test data set usually points to over-fitting.\n",
    "\n",
    "A test set is therefore a set of examples used only to assess the performance (i.e. generalization) of a fully specified classifier. To do this, the final model is used to predict classifications of examples in the test set. Those predictions are compared to the examples' true classifications to assess the model's accuracy.\n",
    "\n",
    "In a scenario where both validation and test datasets are used, the test data set is typically used to assess the final model that is selected during the validation process. In the case where the original data set is partitioned into two subsets (training and test datasets), the test data set might assess the model only once (e.g., in the holdout method). Note that some sources advise against such a method. However, when using a method such as cross-validation, two partitions can be sufficient and effective since results are averaged after repeated rounds of model training and testing to help reduce bias and variability.\n",
    "\n",
    "__Validation data set:__\n",
    "\n",
    "A validation data set is a data-set of examples used to tune the hyperparameters (i.e. the architecture) of a classifier. An example of a hyperparameter for artificial neural networks includes the number of hidden units in each layer. (see course \"Big Data\" in 3ITF) It, as well as the testing set (as mentioned below), should follow the same probability distribution as the training data set.\n",
    "\n",
    "In order to avoid overfitting, when any classification parameter needs to be adjusted, it is necessary to have a validation data set in addition to the training and test datasets. For example, if the most suitable classifier for the problem is sought, the training data set is used to train the different candidate classifiers, the validation data set is used to compare their performances and decide which one to take and, finally, the test data set is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure, and so on. The validation data set functions as a hybrid: it is training data used for testing, but neither as part of the low-level training nor as part of the final testing. \n",
    "\n",
    "__Summary:__\n",
    "\n",
    "You knew this, but remains the question: how do you asses the quality of a model?\n",
    "\n",
    "In the following we'll always make calculations on a set. This set is the validation or the test set, depending on what stage of decision-making we are in. Never the data set, as it was used to create the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The confusion matrix\n",
    "\n",
    "![](files/2023-06-14-16-11-45.png)\n",
    "\n",
    "This is a very simple confusion matrix for a binary classifier. You can have a confusion matrix for multiclass-models to, they just look more complicated. And just to be clear, these are examples in that very same matrix:\n",
    "\n",
    "![](files/confusion_matrix_explained.png)\n",
    "\n",
    "And the terminology:\n",
    "* True positive (TP) Predicted a cat and it was a cat\n",
    "* False positive (FP) Predicted not a cat and it was a cat\n",
    "* False negative (FN) Predicted a cat and it was not a cat\n",
    "* True negative (TN) Predicted not a cat and it was not a cat\n",
    "\n",
    "Building a confusion matrix is easy, but interpreting isn't. As the AWS-ppt states, which of the following is a better model?\n",
    "\n",
    "![](files/2023-06-14-16-27-07.png)\n",
    "\n",
    "The model on the left gets 92 images wrong, the one on the right 81. But the one on the right labels way to much stuff as cats. Is that a problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data for these models:\n",
    "\n",
    "left = { \"TP\": 107, \"FP\": 23, \"TN\": 42, \"FN\": 69 }\n",
    "right = { \"TP\": 148, \"FP\": 53, \"TN\": 12, \"FN\": 28 }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity\n",
    "\n",
    "Sensitivity, also known as recall or true positive rate, is a metric used to evaluate the performance of a classification model, particularly in the context of a confusion matrix. Sensitivity measures the proportion of actual positive instances that are correctly identified by the model.\n",
    "\n",
    "In a confusion matrix, sensitivity is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN). Mathematically, it can be expressed as:\n",
    "\n",
    "Sensitivity = TP / (TP + FN)\n",
    "\n",
    "To understand this further, let's consider the elements of a confusion matrix:\n",
    "\n",
    "- True Positives (TP): The number of instances that are actually positive and are correctly predicted as positive by the model.\n",
    "- False Negatives (FN): The number of instances that are actually positive but are incorrectly predicted as negative by the model.\n",
    "\n",
    "By dividing the number of true positives by the sum of true positives and false negatives, sensitivity quantifies the model's ability to correctly identify positive instances.\n",
    "\n",
    "A high sensitivity value indicates that the model is effective at capturing positive instances and minimizing false negatives. Conversely, a low sensitivity value suggests that the model is missing a significant number of positive instances, resulting in a higher number of false negatives.\n",
    "\n",
    "Sensitivity is particularly relevant in situations where correctly identifying positive instances is crucial. For example, in medical diagnosis, sensitivity is important to minimize the chances of false negatives, ensuring that individuals with a condition are correctly identified and receive appropriate care. If someone is diagnosed with an illness they don't have they might receive some treatment, but along the road it will be discovered that they were healthy, which is better news than the opposite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6079545454545454 0.8409090909090909\n"
     ]
    }
   ],
   "source": [
    "left_sensitivity = left[\"TP\"] / (left[\"TP\"] + left[\"FN\"])\n",
    "right_sensitivity = right[\"TP\"] / (right[\"TP\"] + right[\"FN\"])\n",
    "\n",
    "print(left_sensitivity, right_sensitivity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right sensitivity is higher. This would be good in the following situation: you think your cat may be a rat. You take it to the veterinarian, and he uses this model to asses whether or not to put it down (when it's a rat) or keep it alive (when it's a cat).\n",
    "\n",
    "A false positive would give you the following call: \"You're going to bring in your cat, as it's a rat and I'm going to have to put down anyway.\"\n",
    "\n",
    "A false negative would give you this, less nice call: \"You know the rat we put down the other day? It was in fact a cat. Sorry.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specificity\n",
    "\n",
    "Specificity, also known as true negative rate, is another metric used to evaluate the performance of a classification model, especially in the context of a confusion matrix. Unlike sensitivity, which focuses on positive instances, specificity measures the proportion of actual negative instances that are correctly identified by the model.\n",
    "\n",
    "In a confusion matrix, specificity is calculated as the ratio of true negatives (TN) to the sum of true negatives and false positives (FP). Mathematically, it can be expressed as:\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "Let's review the components of a confusion matrix:\n",
    "\n",
    "- True Negatives (TN): The number of instances that are actually negative and are correctly predicted as negative by the model.\n",
    "- False Positives (FP): The number of instances that are actually negative but are incorrectly predicted as positive by the model.\n",
    "\n",
    "By dividing the number of true negatives by the sum of true negatives and false positives, specificity quantifies the model's ability to correctly identify negative instances.\n",
    "\n",
    "A high specificity value indicates that the model is effective at capturing negative instances and minimizing false positives. It means that the model is accurately recognizing instances that are truly negative. On the other hand, a low specificity value suggests that the model is misclassifying a significant number of negative instances as positive, resulting in a higher number of false positives.\n",
    "\n",
    "Specificity is particularly relevant in scenarios where correctly identifying negative instances is crucial. For example, in a security system, high specificity is important to minimize false alarms and accurately identify instances that are truly negative or benign.\n",
    "\n",
    "In summary, while sensitivity focuses on correctly identifying positive instances, specificity evaluates the model's ability to correctly identify negative instances. Both metrics are essential for assessing the overall performance of a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6461538461538462 0.18461538461538463\n"
     ]
    }
   ],
   "source": [
    "left_specificity = left[\"TN\"] / (left[\"TN\"] + left[\"FP\"])\n",
    "right_specificity = right[\"TN\"] / (right[\"TN\"] + right[\"FP\"])\n",
    "\n",
    "print(left_specificity, right_specificity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both specificities are bad, but the right one is awful. (But remember we had a good sensitivity there.)\n",
    "\n",
    "Suppose you find a lovely looking snake on the side of the road. You want to keep it as a pet, but just to be sure you take it to the vet who uses his \"Safe snake\"-ai. He uploads a picture an based on the answer you take it home (when it's safe) or the vetrenarian keeps it and donates it to the zoo.\n",
    "\n",
    "A false negative would mean the vet calls to say: \"The snake we donated was actually a perfectly safe snake. The other snakes at the zoo ate it though, so you can't have it back.\"\n",
    "\n",
    "A false positive would mean the vet leaves the following message on your answer-service: \"As you may have noticed when the snake bit you and/or your relatives died, she was actually very dangerous. Could you drop it of at the zoo?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholds\n",
    "\n",
    "Basic models, like decision trees and random forests, return a binary (or multiclass) answer (e.g. 'It's a cat' or 'it's a dog'). More advanced models, like XGBoost and linear learner, and all deep learning models used for vision, return a probability. Something like...\n",
    "\n",
    "| Animal | Probability|\n",
    "|--------|-----------|\n",
    "| Cat    | 0,999898  |\n",
    "| Dog    | 0,012352 |\n",
    "| Rabbit | 0,032493  |\n",
    "| Snake  | 0,000243  |\n",
    "\n",
    "This is a clear cut example, but what if it's a very catlike rabbit, or a rabbitlike cat?\n",
    "\n",
    "![](files/2023-06-15-09-35-37.png)\n",
    "![](files/2023-06-15-09-36-12.png)\n",
    "\n",
    "(Some stable diffusion may have been used in the creating of this course material.)\n",
    "\n",
    "Now what? Moving the thresholds will immediately change the sensitivity and the specificity, even without re-evaluating the datasets. This could be a quick win.\n",
    "\n",
    "For example, when putting down cat/rats, make sure you are really sure it's a rat. And when assessing whether a snake is poisonous, make sure it really is safe before letting someone take it home.\n",
    "\n",
    "Maybe we could write a for-loop using every possible threshold and map out these values, allowing us to choose the best? Great idea, but we're not the first thinking of that."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC\n",
    "\n",
    "A receiver operating characteristic (ROC) graph summarizes all the confusion matrices that each threshold produced. To build one, you calculate the sensitivity (or true-positive rate) against the false-positive rate for each threshold value (perhaps going over them with a for-loop?). You then plot this value on a graph. You can calculate the false-positive rate by subtracting the specificity from 1. When those points are plotted, you can draw a line between them.\n",
    "\n",
    "![](files/2023-06-15-09-50-04.png)\n",
    "\n",
    "There are some extreme thresholds:\n",
    "\n",
    "* Everything is a cat (threshold of 0)\n",
    "    * Even a snake is 0.000000001% cat (they both have eyes, for example), so we categorize any image as a cat.\n",
    "    * The point (0,0) on the graph, lower left corner.\n",
    "    * Example: all rats are cats and you get to take them home.\n",
    "* There are no cats (threshold of 1)\n",
    "    * No model will ever be 100% sure about a picture, so we categorize all images as not cat.\n",
    "    * The point (1,1) on the graph, the upper right corner.\n",
    "    * Example: all snakes are dangerous and they all go to the zoo.\n",
    "* The best possible model\n",
    "    * The models labels all images correctly\n",
    "    * The point (0,1) on the graph, the upper left corner.\n",
    "* The worst possible model\n",
    "    * The model labels all cats as other and all other animals as cats.\n",
    "    * The point (1,0) on the graph, the lower right corner.\n",
    "    * Actually a really good model, because when you change the labeling you have the same model as above.\n",
    "* The real worst possible model\n",
    "    * A random classifier: simply guess randomly at each image.\n",
    "    * ROC-curve is the dotted black line in the graph.\n",
    "    * Model has no discriminating power what so ever.\n",
    "\n",
    " The ROC curve does not directly determine the threshold, but it provides information on the model's performance at different thresholds, helping you make an informed decision on selecting a threshold that aligns with your desired balance between sensitivity and specificity.\n",
    "\n",
    "The choice of threshold depends on the specific requirements and trade-offs of the problem at hand. A higher threshold may increase specificity (reduce false positives), but it can also decrease sensitivity (increase false negatives). Conversely, a lower threshold may increase sensitivity (reduce false negatives), but it can also decrease specificity (increase false positives).\n",
    "\n",
    "The ROC curve helps in understanding this trade-off. Points on the curve closer to the top-left corner indicate better trade-offs between sensitivity and specificity, while points closer to the diagonal line represent poorer performance. You can choose a threshold based on the desired balance between true positive and false positive rates, depending on the specific application and the associated costs or consequences of misclassification.\n",
    "\n",
    "For example, if correctly identifying positive instances is more important (higher sensitivity) and a few false positives are tolerable, you might choose a threshold corresponding to a point on the ROC curve with higher sensitivity, even if it means accepting a higher false positive rate. On the other hand, if avoiding false positives is critical, you might select a threshold that results in lower sensitivity but also lower false positive rates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC-ROC\n",
    "\n",
    "So we know we can use the ROC-curve to find the optimal threshold for our application, but how do we know the model is good? Maybe we would be better of using linear learner in stead of XGBoost? Or we could tune some of the hyperparameters to get better results in both of them? Let's retake our steps in the form of simple python code. (The code won't run, but it may be easier to understand than flat text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#region function definitions\n",
    "\n",
    "def load_data_from_file(file_name):\n",
    "    pass\n",
    "\n",
    "def split_data(data, size_1, size_2, size_3):\n",
    "    pass\n",
    "\n",
    "def train_model(data, model_type):\n",
    "    pass\n",
    "\n",
    "def test_model(data):\n",
    "    pass\n",
    "\n",
    "def assess_model(data):\n",
    "    pass\n",
    "\n",
    "def create_ROC(model, data):\n",
    "    pass\n",
    "\n",
    "def calculate_AUC(ROC):\n",
    "    pass\n",
    "\n",
    "#endregion\n",
    "data = load_data_from_file(\"data.csv\")\n",
    "train_data, test_data, validation_data = split_data(data, 70, 15, 15)\n",
    "result = \"not good enough\"\n",
    "all_models = []\n",
    "\n",
    "for model_type in [\"linear\", \"logistic\", \"tree\", \"forest\"]:\n",
    "    while result != \"great!\":\n",
    "        model = train_model(train_data, model_type)\n",
    "        test_data = test_model(model, test_data)\n",
    "        result = assess_model(test_data)\n",
    "    \n",
    "    all_models.append(model)        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left with a list containing a number of models. They all have ROC-curves, but how do they compare? Which model is the best to use in general, without diving deeper into the specific threshold we could be using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in all_models:\n",
    "    roc = create_ROC(model, validation_data)\n",
    "    auc = calculate_AUC(roc)\n",
    "    print(auc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done using the area under the ROC-curve (area under the curve-receiver operator curve ). It's a simple metric saying how much of the surface area is below the curve. This is given as a percentage.\n",
    "\n",
    "* A percentage of 100% would be an ROC-curve that fills the entire surface, a perfect model.\n",
    "* A percentage of 50% would be an ROC-curve that exactly follows the diagonal, which is the worst model possible (a random estimator).\n",
    "* A percentage of 0% would be another perfect model with bad labeling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Yet another measure is accuracy. Where precision and specificity are good measures, they both only use 2 of the 4 values in the confusion matrix. Using all four would be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity:\t0.6079545454545454\t0.8409090909090909\n",
      "Specificity:\t0.6461538461538462\t0.18461538461538463\n",
      "Accuracy:\t0.6182572614107884\t0.6639004149377593\n"
     ]
    }
   ],
   "source": [
    "left_accuracy = (left[\"TP\"] + left[\"TN\"]) / (left[\"TP\"] + left[\"TN\"] + left[\"FP\"] + left[\"FN\"])\n",
    "right_accuracy = (right[\"TP\"] + right[\"TN\"]) / (right[\"TP\"] + right[\"TN\"] + right[\"FP\"] + right[\"FN\"])\n",
    "\n",
    "print(\"Sensitivity:\", left_sensitivity, right_sensitivity, sep=\"\\t\")\n",
    "print(\"Specificity:\", left_specificity, right_specificity, sep=\"\\t\")\n",
    "print(\"Accuracy:\", left_accuracy, right_accuracy, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you notice the accuracy gives quite a good general view of your model. But beware, the right model has a low specificity, meaning it classified 28 cats as not cat and only got 12 not-cats correct. Accuracy misses this.\n",
    "\n",
    "(That may or may not be a problem, depending on your business case.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "When false positives are a problem, precision is a better metric. It calculates as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity:\t0.6079545454545454\t0.8409090909090909\n",
      "Specificity:\t0.6461538461538462\t0.18461538461538463\n",
      "Accuracy:\t0.6182572614107884\t0.6639004149377593\n",
      "Precision:\t0.823076923076923\t0.736318407960199\n"
     ]
    }
   ],
   "source": [
    "left_precision = left[\"TP\"] / (left[\"TP\"] + left[\"FP\"])\n",
    "right_precision = right[\"TP\"] / (right[\"TP\"] + right[\"FP\"])\n",
    "\n",
    "print(\"Sensitivity:\", left_sensitivity, right_sensitivity, sep=\"\\t\")\n",
    "print(\"Specificity:\", left_specificity, right_specificity, sep=\"\\t\")\n",
    "print(\"Accuracy:\", left_accuracy, right_accuracy, sep=\"\\t\")\n",
    "print(\"Precision:\", left_precision, right_precision, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"it's a safe snake!\"-model we would be looking to maximize this metric. It's ok that some safe snakes go to the zoo as long as no dangerous snakes go home with people."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1-score\n",
    "\n",
    "Consider the following confusion matrix:\n",
    "\n",
    "| | Predicted Positive  |  Predicted Negative              |\n",
    "|--| -------------------|----------------------------------|\n",
    "| Actual Positive     |   TP = 80        |       FN = 20  |\n",
    "| Actual Negative     |   FP = 15        |       TN = 285 |\n",
    "\n",
    "What do the numbers say about that?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity:\t0.8\n",
      "Specificity:\t0.95\n",
      "Accuracy:\t0.9125\n",
      "Precision:\t0.8421052631578947\n"
     ]
    }
   ],
   "source": [
    "third = { \"TP\": 80, \"FP\": 15, \"TN\": 285, \"FN\": 20 }\n",
    "\n",
    "third_sensitivity = third[\"TP\"] / (third[\"TP\"] + third[\"FN\"])\n",
    "third_specificity = third[\"TN\"] / (third[\"TN\"] + third[\"FP\"])\n",
    "third_accuracy = (third[\"TP\"] + third[\"TN\"]) / (third[\"TP\"] + third[\"TN\"] + third[\"FP\"] + third[\"FN\"])\n",
    "third_precision = third[\"TP\"] / (third[\"TP\"] + third[\"FP\"])\n",
    "\n",
    "print(\"Sensitivity:\", third_sensitivity, sep=\"\\t\")\n",
    "print(\"Specificity:\", third_specificity, sep=\"\\t\")\n",
    "print(\"Accuracy:\", third_accuracy, sep=\"\\t\")\n",
    "print(\"Precision:\", third_precision, sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the metrics seem fine, but there were 300 negatives and only 100 positives in your dataset. That means that a model that leans towards saying 'no' will have an advantage. Hence the low precision, but high accuracy.\n",
    "\n",
    "The F1-score combines both into, yet another, metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8205128205128205\n"
     ]
    }
   ],
   "source": [
    "third_f1 = 2 * (third_precision * third_sensitivity) / (third_precision + third_sensitivity)\n",
    "\n",
    "print(third_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So which one de we choose? The choice of model metric depends on the specific context, goals, and requirements of the problem at hand. Different metrics serve different purposes, and the most suitable metric may vary across applications and domains. However, some commonly used metrics in model evaluation include accuracy, precision, sensitivity, specificity, F1 score, and area under the ROC curve (AUC-ROC).\n",
    "\n",
    "The selection of a model metric depends on various factors:\n",
    "\n",
    "* Imbalance in the dataset: If the dataset is imbalanced, where one class significantly outweighs the other, metrics like accuracy may not provide an accurate assessment of the model's performance. In such cases, metrics like precision, recall, F1 score, or AUC-ROC are often preferred, as they account for the imbalanced nature of the data.\n",
    "\n",
    "* Application requirements: The choice of metric often depends on the specific requirements of the application. For example, in medical diagnosis, recall may be prioritized to ensure a high true positive rate, while in fraud detection, precision may be more critical to minimize false positives. Understanding the application's needs and potential consequences of false positives and false negatives helps in selecting the appropriate metric.\n",
    "\n",
    "* Trade-off between precision and specificity: The F1 score is commonly used when achieving a balance between precision and recall is important. It provides a single measure that considers both metrics and is suitable when the costs of false positives and false negatives are similar.\n",
    "\n",
    "* Model comparison: When comparing multiple models, accuracy and AUC-ROC are frequently used metrics. Accuracy is straightforward to interpret and often provides a general assessment of model performance. AUC-ROC, on the other hand, evaluates the model's ability to discriminate between classes across various thresholds, making it suitable for model comparison and ranking.\n",
    "\n",
    "Ultimately, there is no one-size-fits-all metric, and the choice depends on the specific goals and requirements of the problem. It's common to evaluate models using multiple metrics to gain a comprehensive understanding of their performance and to ensure that the selected model aligns with the desired objectives."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "And before we forget: when doing regression and not classification, the go-to metric is RMSE. But you know all about that from the chapter before this one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

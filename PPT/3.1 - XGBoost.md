# XGBoost

__Note: this chapter assumes some previous knowledge about AI, but also repeats some concepts of other AI-courses. If a part is not entirely clear, do look it up in the other courses. The goal is to lead you past the stuff you know to understand how XCBoost works and came to be.__

XGBoost[2] (eXtreme Gradient Boosting) is an open-source software library which provides a regularizing gradient boosting framework. From the project description, it aims to provide a "Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library". ([link](https://en.wikipedia.org/wiki/XGBoost))

But what is Gradient Boosting? Well, the story starts earlier, with decision trees.

## Decision trees

look at the following image:

![](files/ten1.png)

Will John play tennis tomorrow? To decide we look at the forecast and compare the parameters (outlook, humidity, wind) to decide this. The structured way would be to ask a number of simple questions. But which questions?

![](files/ten4.png)

This is a decision tree. For this data it allows us to accurately predict whether or not John will be playing tennis.

## Random forests

Decision trees are very simple, they are not flexible when it comes to classify new samples. Random Forests combine the simplicity of decision trees with flexibility resulting in a vast improvement in accuracy.

Watch the following video (from 0:25 till 9:27) to see how we can improve the accuracy of our predictions using a random forest.

[Link](https://www.youtube.com/embed/J4Wdy0Wc_xQ?start=24&end=567)

But we were working towards Gradient Boosting, not random forest.

## Gradient boosting

Gradient boosting is a machine learning technique used in regression and classification tasks, among others. It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees. When a decision tree is the weak learner, the resulting algorithm is called gradient-boosted trees; it usually outperforms random forest. A gradient-boosted trees model is built in a stage-wise fashion as in other boosting methods, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function. ([link](https://en.wikipedia.org/wiki/Gradient_boosting))

There is one thing to note: Decision trees and random forest do classification, not regression. The classification can be binary or multiclass, but neither decision trees or random forest will ever tell you that they are 98.72482% shure that John will be playing tennis. Gradient boosting will, as the output is suited for not just classification, but also for regression.

But how does it work? Look at [this](https://www.youtube.com/watch?v=en2bmeB4QUo) video.

It has a lot of math, and difficult math at that (derivatives). What you need to know (or remember from your high-school math) is that a derivative or gradient (dutch: afgeleide) of a function gives you the direction in which that function goes on a certain point. You don't need to know or remember the math, but you should be able to explain that XGBoost combines weak learners (such as decision trees, but other models could be used) using a mathematical function to combine them in the best way possible.


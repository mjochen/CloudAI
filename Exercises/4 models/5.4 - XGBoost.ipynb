{"cells": [{"cell_type": "markdown", "id": "366b79a3", "metadata": {}, "source": ["# eXtreme Gradient Boosting\n", "\n", "Gradient boosting let us down a bit, having an RMSE that was higher than the random forest. Maybe extreme gradient boosting would be better?"]}, {"cell_type": "markdown", "id": "582f8736", "metadata": {}, "source": ["## Data import\n", "\n", "We exported the data before into a pickle-file. That means we can quite simply import it here again."]}, {"cell_type": "code", "execution_count": null, "id": "8066114c", "metadata": {}, "outputs": [], "source": ["import pickle\n", "\n", "# Load the pickle file\n", "with open('../exports/non_linear_data.pkl', 'rb') as file:\n", "    data_dict = pickle.load(file)\n", "\n", "# Display the loaded data\n", "\n", "X_train = data_dict[\"X_train\"]\n", "X_test = data_dict[\"X_test\"]\n", "y_train = data_dict[\"y_train\"]\n", "y_test = data_dict[\"y_test\"]"]}, {"cell_type": "markdown", "id": "bbda65f8", "metadata": {}, "source": ["## Create model\n", "\n", "With all the data ready in files, creating and training the model shouldn't be a problem. And because we've seen the pattern of \"train, tune, train\" a couple of times now, we'll simply start by tuning.\n", "\n", "Use a gridsearch with the following grid:\n", "\n", "```Python\n", "param_grid = {\n", "    'n_estimators': [50, 100, 200],\n", "    'max_depth': [3, 5, 7],\n", "    'learning_rate': [0.01, 0.1, 0.2],\n", "    'subsample': [0.6, 0.8, 1.0],\n", "    'colsample_bytree': [0.6, 0.8, 1.0],\n", "    'gamma': [0, 0.1, 0.2]\n", "}\n", "```\n", "When the gridsearch is done use the parameters to create and train the 'final' model.\n", "\n", "When you run it, you'll get the following message:\n", "\n", "![](../files/2025-05-10-12-00-43.png)\n", "\n", "And still it won't take long enough to get a coffee."]}, {"cell_type": "code", "execution_count": null, "id": "9a73daef", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "98e11416", "metadata": {}, "source": ["Nothing much more to do here. Since we started out by tuning the model immediatly went down to 2.082, which is lower that the 2.084 we got from a random forest.\n", "\n", "Not much more to do but export the data!"]}, {"cell_type": "code", "execution_count": null, "id": "69c495db", "metadata": {}, "outputs": [], "source": ["with open('../exports/y_pred_xgboost.pkl', 'wb') as f:\n", "    pickle.dump(y_pred, f)"]}], "metadata": {"kernelspec": {"display_name": "venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.5"}}, "nbformat": 4, "nbformat_minor": 5}
{"cells": [{"cell_type": "markdown", "id": "bf32b0bf", "metadata": {}, "source": ["# Selection bias\n", "\n", "One of the types of bias that can be handled with technology is selection bias. The cover forest-dataset has a good example of that."]}, {"cell_type": "markdown", "id": "344a4e0f", "metadata": {}, "source": ["## Data import\n", "\n", "This dataset can be fetched by using sklearn. We'll import it and paste X and y into one dataframe."]}, {"cell_type": "code", "execution_count": null, "id": "66455ec3", "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import fetch_covtype\n", "\n", "# Load the Covertype dataset\n", "data = fetch_covtype(as_frame=True)\n", "X = data.data\n", "y = data.target\n", "\n", "print(X.head())\n", "print(y.head())\n", "\n", "df = X.copy()\n", "df['target'] = y"]}, {"cell_type": "markdown", "id": "90152102", "metadata": {}, "source": ["## Detecting selection bias\n", "\n", "The easiest way of detecting selection bias is seeing how many rows there are per label in the target column.\n", "\n", "Show the value_counts, and add a percentage which this label is of the total."]}, {"cell_type": "code", "execution_count": null, "id": "5a8804f4", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "c53aa298", "metadata": {}, "source": ["This shows that the situation is quite dire. Almost 50% is of class 2, and only 0.5% is of class 4. A classic case of class imbalance or selection bias. Let's fix it!"]}, {"cell_type": "markdown", "id": "dcbf103c", "metadata": {}, "source": ["## Get more data\n", "\n", "If possible this is the best approach. Make sure you have more data on the classes which have lower samples. But in this case it's not a possibility."]}, {"cell_type": "markdown", "id": "9dcd5d2b", "metadata": {}, "source": ["## Use Algorithms that Handle Imbalance Naturally\n", "\n", "Some algorithms are more robust to imbalance Gradient Boosted Trees (e.g., LightGBM, CatBoost, XGBoost with scale_pos_weight), or a BalancedBaggingClassifier or BalancedRandomForestClassifier. These algorithms handle the imbalance without us doing any more work."]}, {"cell_type": "markdown", "id": "daa41103", "metadata": {}, "source": ["## Use Class Weights\n", "\n", "Set class weights inversely proportional to class frequencies: For logistic regression, SVM, RandomForest, XGBoost, etc., you can use the class_weight='balanced' argument (or manually compute weights). This is often the simplest and most robust first approach, especially for tree-based models.\n", "\n", "Example in sklearn:\n", "```Python\n", "from sklearn.ensemble import RandomForestClassifier\n", "clf = RandomForestClassifier(class_weight='balanced')\n", "```\n", "\n", "Let's try this! First split the data in a train/validation/test set. Let's say 60/20/20. Make sure you do a stratified split, so all labels are equally represented in training, test and validation."]}, {"cell_type": "code", "execution_count": null, "id": "df0d028c", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "84392d6d", "metadata": {}, "source": ["Show the distribution of labels in the test-set (along with percentages) to check if the stratification worked."]}, {"cell_type": "code", "execution_count": null, "id": "d3be4c98", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "a344baf4", "metadata": {}, "source": ["Now train a RandomForestClassifier (using the training-set). Don't use the \"class_weight\" yet. Also show the accuracy_score."]}, {"cell_type": "code", "execution_count": null, "id": "7b919c1e", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "e79d1360", "metadata": {}, "source": ["Now train a RandomForestClassifier (using the training-set) but use the \"class_weight\" this time. Show the accuracy_score and don't forget to give your model a different name!"]}, {"cell_type": "code", "execution_count": null, "id": "dee2e4a2", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "bc48a0ef", "metadata": {}, "source": ["Accuracy when using weighted: 0.0008% better. Not really the gains we were hoping for. But maybe we are getting the gains we wan't but just in the underrepresented labels. \n", "\n", "Show the confusion matrix and classification_report for the weighted and unweighted results."]}, {"cell_type": "code", "execution_count": null, "id": "08d93e5e", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "0d784e2c", "metadata": {}, "source": ["The difference is marginal at best. At least it doesn't make the model less good (as it would in an untuned decision tree).\n", "\n", "There is a special random forest implementation for dealing with unbalanced data ([here](https://imbalanced-learn.org/stable/references/generated/imblearn.ensemble.BalancedRandomForestClassifier.html)). Let's try that one!\n", "\n", "First install the library..."]}, {"cell_type": "code", "execution_count": null, "id": "c8d8d9d0", "metadata": {}, "outputs": [], "source": ["# !pip install imbalanced-learn"]}, {"cell_type": "markdown", "id": "97ccbbfd", "metadata": {}, "source": ["And create and train the model on the existing split."]}, {"cell_type": "code", "execution_count": null, "id": "4c7b6c3b", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "6f7b9371", "metadata": {}, "source": ["Accuracy went down further, dropping all the way below 80% now. And what do the confusion matrix and classification report say?"]}, {"cell_type": "code", "execution_count": null, "id": "8acd34b7", "metadata": {}, "outputs": [], "source": ["# Unweighted model\n", "# Compute the confusion matrix\n", "conf_matrix = confusion_matrix(y_test, y_pred)\n", "# Display the confusion matrix\n", "print(\"Confusion Matrix unweighted model:\")\n", "print(conf_matrix)\n", "\n", "print(classification_report(y_test, y_pred_weighted_v2))"]}, {"cell_type": "markdown", "id": "46445b60", "metadata": {}, "source": ["As expected, bad news all around. Precision even drops to 31% for class 5.\n", "\n", "This can be due to a number of reasons:\n", "* It Undersamples the Majority Class (Aggressively): Randomly undersamples each majority class within each tree, so that classes are balanced. This can throw away a lot of useful majority-class data, reducing the model's capacity to learn overall patterns. Especially on a large dataset like the Forest Cover dataset, this can harm overall accuracy and generalization unless tuned well.\n", "* It Needs More Trees to Be Stable: Because each tree sees only a small, randomly sampled (and heavily undersampled) portion of the data, you often need more trees to average out the noise.\n", "* It\u00e2\u20ac\u2122s Sensitive to Tree Depth and Leaf Size: By default, it uses deep trees, which overfit the small, undersampled data and hurt generalization, especially on the majority classes.\n", "\n", "So this is not the end of this story, but the beginning: tune all three models and see where you can get. Use the validation-set when you're happy with the model to get a final reading on the accuracy.\n", "\n", "But for now, let's look at other ways of dealing with the imbalance."]}, {"cell_type": "markdown", "id": "f42fd9df", "metadata": {}, "source": ["## Resampling the Dataset\n", "\n", "Resampling means working with the samples we already have. We have two options:\n", "\n", "1. Oversampling minority classes\n", "    * SMOTE (Synthetic Minority Over-sampling Technique): Generates synthetic examples.\n", "    * Random Oversampling: Simply duplicates examples from minority classes.\n", "1. Undersampling majority classes\n", "    * Random Undersampling: Drops examples from majority classes.\n", "    * Tomek Links / Edited Nearest Neighbors: Smarter undersampling using data geometry.\n", "\n", "But before we begin, beware: over- and undersampling work for tree-based models, but there is a risk of overfitting (models training on a small part of the dataset) or losing information (when dropping rows when doing undersampling).\n", "\n", "First apply SMOTE (from imblearn, which we installed earlier) to the training dataset. Then use \"Counter\" from collections to check the amount of rows in every categorie."]}, {"cell_type": "code", "execution_count": null, "id": "a1cf88a2", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "11f61875", "metadata": {}, "source": ["Now train a new random forest classifier. Don't use any tuning parameters (n_estimators, ...), only the default random state of 42. Give it a new name when fitting and don't overwrite the y_pred results we had earlier.\n", "\n", "Remember that training the original dataset took about 2 minutes. Now we have a much, much bigger dataset.\n", "\n", "While waiting, here's something to think about: we only resampled the training-set, not the test-set. Why not? What would have happened to our precision had we done that?"]}, {"cell_type": "code", "execution_count": null, "id": "27493328", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "80904455", "metadata": {}, "source": ["A (slightly, but still) better model than without using smote! let's look at all the specs first."]}, {"cell_type": "code", "execution_count": null, "id": "c6bb3582", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "0f226326", "metadata": {}, "source": ["Precision is sometimes down on the previous model, but general accuracy is up. Once again we didn't find a silver bullet to fix all, but I hope that idea is gone by now: it doesn't exist. Machine learning is not magic but hard work, tinkering with the dataset, trying new tools and stopping before going too far. But this small increase is a good sign: start tuning and you're on to something.\n", "\n", "Next thing to try is undersampling. We could do a random undersampling, but that would lead to roughly the same results as the imblearn version of the random forest classifier. In stead of that we'll try two of the other undersampling methods: Tomek links and ENN.\n", "\n", "First import TomekLinks from imblearn and use it to resample the training-data."]}, {"cell_type": "code", "execution_count": null, "id": "867921d5", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "f4116f9d", "metadata": {}, "source": ["Look at the numbers and decide: do we really need to train a new model on this data? Will it be any better?\n", "\n", "It won't. Tomek Links work by looking at the data and removing datapoints based on their distance. [read more](https://imbalanced-learn.org/stable/under_sampling.html#tomek-links). Apparantly there are not enough of these to delete, so the dataset remains roughly the same.\n", "\n", "But hey, we're not doing deep learning, so let's train another model."]}, {"cell_type": "code", "execution_count": null, "id": "cd73dfc5", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "32a8a70f", "metadata": {}, "source": ["And once more for ENN!"]}, {"cell_type": "code", "execution_count": null, "id": "17e1a350", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "56aa25d6", "metadata": {}, "source": ["(You know the drill by now.)"]}, {"cell_type": "code", "execution_count": null, "id": "40eff980", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "b77faadb", "metadata": {}, "source": ["When to use these techniques:\n", "* Tomek Links: Cleaning borderline cases, especially in combo with oversampling (like SMOTE + Tomek)\n", "* ENN: Removing noisy samples, cleaning clusters; can be aggressive and may underfit if data is sparse\n", "\n", "And how would you combine these?"]}, {"cell_type": "code", "execution_count": null, "id": "dac118af", "metadata": {}, "outputs": [], "source": ["from imblearn.combine import SMOTETomek, SMOTEENN\n", "\n", "# smote_tomek = SMOTETomek(random_state=42)\n", "# X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n", "\n", "# OR\n", "# smote_enn = SMOTEENN(random_state=42)\n", "# X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n"]}, {"cell_type": "markdown", "id": "c1627543", "metadata": {}, "source": ["... but since we seem to have decided to stay away from tuning for now, let's not dig deeper there."]}, {"cell_type": "markdown", "id": "9a00431d", "metadata": {}, "source": ["## Anomaly Detection Perspective\n", "\n", "If one or more classes are extremely rare (like class 4 in this case), treat them as anomalies. We could use one-vs-rest classifiers and train models to detect \"normal\" vs \"abnormal\" classes.\n", "\n", "Let's start by looking at the original data distribution once again."]}, {"cell_type": "code", "execution_count": null, "id": "f457df10", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "0c2837c9", "metadata": {}, "source": ["It looks like class 3 to 7 are the smallest. What if we were to combine these into one giant class and retrain the model? We'll remap them all to class 3."]}, {"cell_type": "code", "execution_count": null, "id": "cd9254af", "metadata": {}, "outputs": [], "source": ["y_test_mapped = y_test.replace({4: 3, 5: 3, 6: 3, 7: 3})\n", "y_train_mapped = y_train.replace({4: 3, 5: 3, 6: 3, 7: 3})"]}, {"cell_type": "markdown", "id": "1c7ee424", "metadata": {}, "source": ["And train yet another random forest classifier to distinguish in our mapped datasets."]}, {"cell_type": "code", "execution_count": null, "id": "5ec087d2", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "a6cb01c4", "metadata": {}, "source": ["The accuracy is pretty good, amoung the highest we had so far. So now we have a model that says in inference that \"a patch is of type 3 or 4 or 5 or 6 or 7\" so we need another model to distinguish between these.\n", "\n", "Start by getting all rows from the original dataset that map to class 3 or higher. Use the \"df\", that has the X and y combined."]}, {"cell_type": "code", "execution_count": null, "id": "7090a092", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "7f41413f", "metadata": {}, "source": ["Now apply a train/test-split to df (maybe even a validation-set?)"]}, {"cell_type": "code", "execution_count": null, "id": "d5159e0d", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "7b844f91", "metadata": {}, "source": ["Maybe one more model? It'll be faster because it's much smaller."]}, {"cell_type": "code", "execution_count": null, "id": "f4f0a7f5", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "b6089a0d", "metadata": {}, "source": ["We have an accuracy, but it's an accuracy that has to be applied after the previous accuracy has been applied so it's hard to compare.\n", "\n", "What we've been doing is also very tricky: what if classes 3-7 have no relation whatsoever? What if class 3 falls somewhere between 1 and 2 in a multidimensional space? In that case it would have been better to combine 1 and 3 vs 2 and the rest. When going down this rabbit hole be prepared to:\n", "\n", "- Undo all your work and start over\n", "- Keep enough data separate to be able to test your model(s) on unseen data"]}, {"cell_type": "markdown", "id": "17ec5031", "metadata": {}, "source": ["## Extra\n", "\n", "Just because we can: a script that visualizes what smote and tomek do to your data on a subset of 10k rows. It uses as 2D PCA projection to visualize the class distributions in the 54 dimensions of our original data.\n"]}, {"cell_type": "code", "execution_count": null, "id": "8001aa3b", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "from sklearn.datasets import fetch_covtype\n", "from sklearn.decomposition import PCA\n", "from imblearn.combine import SMOTETomek, SMOTEENN\n", "from collections import Counter\n", "import seaborn as sns\n", "\n", "# Load the Forest Cover dataset\n", "X, y = fetch_covtype(return_X_y=True)\n", "\n", "# Optional: Downsample for speed/visualization\n", "from sklearn.model_selection import train_test_split\n", "X_small, _, y_small, _ = train_test_split(X, y, train_size=10000, stratify=y, random_state=42)\n", "\n", "# Reduce to 2D with PCA for visualization\n", "pca = PCA(n_components=2, random_state=42)\n", "X_pca = pca.fit_transform(X_small)\n", "\n", "# Helper: Plot function\n", "def plot_pca(X_proj, y, title):\n", "    plt.figure(figsize=(8, 6))\n", "    sns.scatterplot(x=X_proj[:, 0], y=X_proj[:, 1], hue=y, palette='tab10', s=20, alpha=0.7, linewidth=0)\n", "    plt.title(title)\n", "    plt.xlabel(\"PCA 1\")\n", "    plt.ylabel(\"PCA 2\")\n", "    plt.legend(title=\"Class\", bbox_to_anchor=(1.05, 1), loc='upper left')\n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "# Visualize original\n", "plot_pca(X_pca, y_small, \"Original Dataset (PCA Projection)\")\n", "\n", "# ---- SMOTE + Tomek ----\n", "from imblearn.over_sampling import SMOTE\n", "smote_tomek = SMOTETomek(random_state=42)\n", "X_res_tomek, y_res_tomek = smote_tomek.fit_resample(X_small, y_small)\n", "X_res_tomek_pca = pca.transform(X_res_tomek)\n", "print(\"SMOTE + Tomek class distribution:\", Counter(y_res_tomek))\n", "plot_pca(X_res_tomek_pca, y_res_tomek, \"SMOTE + Tomek Links (PCA Projection)\")\n", "\n", "# ---- SMOTE + ENN ----\n", "smote_enn = SMOTEENN(random_state=42)\n", "X_res_enn, y_res_enn = smote_enn.fit_resample(X_small, y_small)\n", "X_res_enn_pca = pca.transform(X_res_enn)\n", "print(\"SMOTE + ENN class distribution:\", Counter(y_res_enn))\n", "plot_pca(X_res_enn_pca, y_res_enn, \"SMOTE + ENN (PCA Projection)\")\n"]}], "metadata": {"kernelspec": {"display_name": "venv (3.12.5)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.5"}}, "nbformat": 4, "nbformat_minor": 5}
{"cells": [{"cell_type": "markdown", "id": "66568d36", "metadata": {}, "source": ["# Binary classification using yeast\n", "\n", "We'll be using the yeast dataset that is made publicly available by [openml.org](https://www.openml.org). You can read all about this set [here](https://www.openml.org/search?type=data&sort=runs&id=40597&status=active)."]}, {"cell_type": "markdown", "id": "eb847926", "metadata": {}, "source": ["## Data import and exploration\n", "\n", "Let's explore it ourselves. First, import. We could import using the import from the openml-python package, but sklearn provides an easier way. We'll import it that way and explore the size of what we have imported."]}, {"cell_type": "code", "execution_count": null, "id": "ae01729d", "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import numpy as np\n", "from sklearn.datasets import fetch_openml\n", "import pandas as pd\n", "\n", "X, Y = fetch_openml(\"yeast\", version=4, return_X_y=True)\n", "\n", "print(X.shape) \n", "print(Y.shape) "]}, {"cell_type": "markdown", "id": "cc5d8258", "metadata": {}, "source": ["We have 2417 rows. In X, the data, we have 103 attributes and in Y, what we should be predicting we see 14 classes. On the [description-page](https://www.openml.org/search?type=data&sort=runs&id=40597&status=active) of this data we read that only 13 of those are actually used because of label-sparsity (very few examples are available).\n", "\n", "Funniest thing: X and Y are pandas dataframes without us losing any effort over that. Good!\n", "\n", "Look at Y to see what we are predicting."]}, {"cell_type": "code", "execution_count": null, "id": "56b41ac8", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "54b2beb9", "metadata": {}, "source": ["Check the datatypes of the attributes (in X). Only show the different types."]}, {"cell_type": "code", "execution_count": null, "id": "1e63af47", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "7581f844", "metadata": {}, "source": ["Ok, so this learns us we are strictly working with numbers. Maybe check some graphs on some of the number? Boxplots for example!"]}, {"cell_type": "code", "execution_count": null, "id": "95c38148", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "e58e5470", "metadata": {}, "source": ["All seems pretty normal. Some outliers; but we'll leave it at that. Every row can lead to multiple labels (in Y). Show for every column in Y how many rows have that label. Unfortunately they're not stored as a boolean but as a string value."]}, {"cell_type": "code", "execution_count": null, "id": "ab666554", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "db323e27", "metadata": {}, "source": ["Knowing that we have about 2400 rows, Class2 seems pretty close to the middle. Let's predict that!"]}, {"cell_type": "markdown", "id": "65753b6c", "metadata": {}, "source": ["## Binary classifier\n", "\n", "Let's build a binary classifier for Class 2. We'll be using a random forest classifier. (Or you can use another model if you'd like...)"]}, {"cell_type": "code", "execution_count": null, "id": "da095966", "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import classification_report, confusion_matrix\n", "\n", "# X = X\n", "y = Y[\"Class2\"]"]}, {"cell_type": "markdown", "id": "7b5b1fb3", "metadata": {}, "source": ["Now split the dataset in a training and a test-set. Use random state of [42](https://en.wikipedia.org/wiki/42_(number)). Use 20% of the data as test-set."]}, {"cell_type": "code", "execution_count": null, "id": "b0c19a81", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "f92939f0", "metadata": {}, "source": ["Next create the model and train it ons the data. Use 100 estimators and the same random state."]}, {"cell_type": "code", "execution_count": null, "id": "85cdc748", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "c193cc3e", "metadata": {}, "source": ["Evaluate the model by predicting it on the test-set. Also create a confusion matrix and calculate all the scores."]}, {"cell_type": "code", "execution_count": null, "id": "ef60b0fa", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "c22d6e4e", "metadata": {}, "source": ["Precision and recall are about .67 and .83, which isn't great. Note how we can not draw an ROC curve because we simply predicted a label. Luckily there is a way to get the probabilities from random forest by using \"rf.predict_proba(X_test)[:, 1]\". Try it and display the first 10 probabilities."]}, {"cell_type": "code", "execution_count": null, "id": "1d583a78", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "0fd07012", "metadata": {}, "source": ["Now draw the ROC curve."]}, {"cell_type": "code", "execution_count": null, "id": "65d33126", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "ed5a1813", "metadata": {}, "source": ["Now you can see how bad the model is."]}, {"cell_type": "markdown", "id": "990e5cf8", "metadata": {}, "source": ["## Bigger forest\n", "\n", "Using the same train/test split, retrain your model with 500 estimators. Draw the ROC-curves again.\n"]}, {"cell_type": "code", "execution_count": null, "id": "6f6bc05e", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "c0d134cd", "metadata": {}, "source": ["More bubbly but not better. You did notice it took longer, no? Try another model next!"]}, {"cell_type": "markdown", "id": "5542a93a", "metadata": {}, "source": ["## XGBoost\n", "\n", "Now use XGBoost. Use the same train/test-split.\n", "\n", "One problem though: XGBoost works best with \"1\" and \"0\", not with \"TRUE\" and \"FALSE\". Create \"y_train_new\" and \"y_test_new\" based on the existing ones, but with the correct data.\n"]}, {"cell_type": "code", "execution_count": null, "id": "1fdefad8", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "db5591fe", "metadata": {}, "source": ["Now you're ready to train XGboost. (You have to install it first.) "]}, {"cell_type": "code", "execution_count": null, "id": "9f18b252", "metadata": {}, "outputs": [], "source": ["# !pip install xgboost"]}, {"cell_type": "code", "execution_count": null, "id": "368fcc8e", "metadata": {}, "outputs": [], "source": ["import xgboost as xgb\n", "\n", "model = xgb.XGBClassifier(\n", "    objective='binary:logistic',\n", "    eval_metric='logloss',\n", "    use_label_encoder=False,\n", "    random_state=42\n", ")\n", "\n", "model.fit(X_train, y_train_new)\n", "\n", "y_3_pred = model.predict(X_test)\n", "y_3_proba = model.predict_proba(X_test)[:, 1]\n", "\n", "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_new, y_3_pred))\n", "\n", "# print(\"Classification Report:\\n\", classification_report(y_test_new, y_3_pred))\n", "# print(\"AUC Score:\", roc_auc_score(y_test_new, y_3_proba))\n", "\n", "fpr, tpr, _ = roc_curve(y_test_new, y_3_proba)\n", "plt.figure(figsize=(8, 6))\n", "plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc_score(y_test_new, y_3_proba):.2f})')\n", "plt.plot([0, 1], [0, 1], 'k--')\n", "plt.xlabel(\"False Positive Rate\")\n", "plt.ylabel(\"True Positive Rate\")\n", "plt.title(\"ROC Curve\")\n", "plt.legend(loc=\"lower right\")\n", "plt.grid(True)\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "bd003895", "metadata": {}, "source": ["This model is even worse than random forest! That is because when using XGboost hyperparameter tuning is essential. We're now looking at all 103 input parameters equally, which isn't a good idea. Check out \"rf.feature_importances_\" and \"model.get_booster().get_score()\" to see which are the more interesting features.\n", "\n", "Start with \"rf.feature_importances_\"."]}, {"cell_type": "code", "execution_count": null, "id": "26642386", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "d6f37f7d", "metadata": {}, "source": ["You'll see a list of numbers, one for every column. The numbers add up to 1, and bigger numbers mean the feature is more important (in more of the decision trees, but that's diving a bit deep before we looked into what a random forest does).\n", "\n", "What does \"model.get_booster().get_score()\" tell you?\n", "\n", "You can use a modifier in the last bracket, choosing the importance_type. The options are:\n", "\n", "* 'weight' (default): number of times a feature is used to split across all trees.\n", "* 'gain': average gain in accuracy brought by the feature when it is used in a split.\n", "* 'cover': average number of samples affected by the splits using that feature.\n", "* 'total_gain': total gain (sum across all splits).\n", "* 'total_cover': total cover (sum across all splits)."]}, {"cell_type": "code", "execution_count": null, "id": "d5984cbd", "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "id": "7ed75a04", "metadata": {}, "source": ["## Summary\n", "\n", "We'll stop here, as we've already ventured to far. Why? Because we're tuning a model using only a two-set split. That is not allowed, we should have used a three set split (more on that in the next chapter).\n", "\n", "But what have we learned? We've used a couple of models on a large dataset and noticed that results aren't always great from the start. We've also used the results of our model to calculate all the scores we went over in the powerpoint.\n", "\n", "We still didn't start doing data augmentation or parameter tuning. But we'll get there!"]}], "metadata": {"kernelspec": {"display_name": "venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.5"}}, "nbformat": 4, "nbformat_minor": 5}
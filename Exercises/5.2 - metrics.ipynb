{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Metrics\n", "\n", "Let's experiment with the metrics. In the following exercises, the confusion matrices are given and you need to decide which one to use.\n", "\n", "We'll start be defining a class for you to use, as it's easier than a dict in this case."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Some preliminary code\n", "class Confusion_matrix():\n", "\n", "    def __init__(self, tp, tn, fp, fn):\n", "        self.tp = tp\n", "        self.tn = tn\n", "        self.fp = fp\n", "        self.fn = fn\n", "\n", "    def __str__(self):\n", "        return f\"Confusion matrix: \\n\" \\\n", "                f\"TP: {self.tp}\\t| FP: {self.fp} \\n\" \\\n", "                f\"FN: {self.fn}\\t| TN: {self.tn}\"\n", "\n", "cats = Confusion_matrix(107, 42, 23, 69)\n", "print(cats)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Exercise 1\n", "\n", "Consider two classification models, Model A and Model B, trained to detect fraudulent transactions. You have their respective confusion matrices:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["models = {}\n", "models[\"A\"] = Confusion_matrix(150, 280, 50, 20)\n", "models[\"B\"] = Confusion_matrix(200, 260, 40, 40)\n", "\n", "print(f\"Model A: \\n{models['A']}\")\n", "print(f\"\\nModel B: \\n{models['B']}\")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["1. Calculate the accuracy for both models and determine which model has a higher accuracy.\n", "1. Calculate the precision for both models and identify which model has higher precision.\n", "1. Calculate the sensitivity for both models and identify which model has higher sensitivity.\n", "1. Calculate the specificity for both models and identify which model has higher specificity.\n", "1. Calculate the F1 score for both models and determine which model has a higher F1 score.\n", "\n", "Tip: look at [the eval-function](https://realpython.com/python-eval-function/) to keep the copy-pasting to a minimum."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["Now you have numbers, so you can make the following decisions easily. Remember the model was about detecting fraude in transactions.\n", "\n", "1. You are a bank and legally need to have this model in place, but really you don't want to inconvenience your clients to much. Better let some bad ones get through than to annoy our good customers!\n", "1. You are the government and are using this model to weed out the obviously ok transactions. Anything tagged fraudulent here will be investigated and if it is then discovered that it was fine that is no problem, but you really don't want any tax dodgers to get through!\n", "1. You are a student who is graded on the overall performance of this model. Try to find the best performance, balanced between false positives and negatives."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## Exercise 2\n", "\n", "Consider two classification models, Model X and Model Y, trained to diagnose a specific medical condition. You have their respective confusion matrices:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["models = {}\n", "models[\"X\"] = Confusion_matrix(100, 270, 10, 20)\n", "models[\"Y\"] = Confusion_matrix(90, 260, 20, 10)\n", "\n", "print(f\"Model X: \\n{models['X']}\")\n", "print(f\"\\nModel Y: \\n{models['Y']}\")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["1. Calculate the accuracy for both models and determine which model has a higher accuracy.\n", "1. Calculate the precision for both models and identify which model has higher precision.\n", "1. Calculate the sensitivity for both models and identify which model has higher sensitivity.\n", "1. Calculate the specificity for both models and identify which model has higher specificity.\n", "1. Calculate the F1 score for both models and determine which model has a higher F1 score.\n", "\n", "Tip: don't rewrite the functions you made for exercises 1."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Model X: \\n{models['X']}\\n\")\n", "print(f\"Model Y: \\n{models['Y']}\\n\")\n", "\n", "for metric in metrics:\n", "    print(metric, end=\"\\t\")\n", "    for model in models.values():\n", "        print(round(eval(metric)(model),3), end=\"\\t\")\n", "    print()"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["Remember this is a model diagnosing a certain medical condition. Write a scenario for each metric when you would want it maximized.\n", "\n", "Inspiration for scenario's:\n", "* A disease that can only be cured when detected fast\n", "* A disease for which the cure can be dangerous\n", "* An insurance-company looking to minimize cost\n", "* A pharmaceutical company looking to boost the numbers on an illness to secure more funding"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}], "metadata": {"kernelspec": {"display_name": "venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.2"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}
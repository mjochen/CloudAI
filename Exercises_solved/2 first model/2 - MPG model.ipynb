{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model\n",
    "\n",
    "In this notebook we'll implement linear regression to predict the mpg (miles per gallon) based on the parameters in the mpg-dataset. This notebook (and the following python-file containing the streamlit app) are examples. In the next notebook you can apply all you've absorbed here on the titanic dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and cleaning\n",
    "\n",
    "Importing is a basic copy-paste of the previous notebook. We've added the first column as an index in stead of a regular column though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mpg = pd.read_csv('../files/mpg.csv', index_col=0)\n",
    "mpg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll be splitting the data in X (the parameters) and Y (what we want to predict.) First print all column names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And copy and edit the ones we'll be using.\n",
    "\n",
    "We'll remove:\n",
    "\n",
    "- 'cty' and 'hwy' because they are the target variables\n",
    "- 'manufacturer' and 'model' because they are not useful for the model\n",
    "\n",
    " (Stricly speaking only hwy, highway miles per gallon, is the target variable, but cty, city miles per gallon, would be too good a predictor. It would dominate the model.)\n",
    "\n",
    "We'll keep:\n",
    "\n",
    "- 'displ': engine displacement in liters\n",
    "- 'year': year of manufacture\n",
    "- 'cyl': number of cylinders\n",
    "- 'trans': type of tranmission *\n",
    "- 'drv': type of drive train *\n",
    "- 'fl': fuel type *\n",
    "- 'class': type of car *\n",
    "\n",
    "There is one problem though: trans, drv, fl and class are strings, not numbers. This can be fixed by encoding them (compact = 1, suv = 2, ...). That however introduces bias: 1 is numerically less than 2. This implies a relionship that does exist for numercial values (a car built in 1999 is older than a car built in 2004), but not in type of car (1 may be smaller and lighter than 2, but that relationship won't hold up for 2 and 3.)\n",
    "\n",
    "A solution would be on hot encoding, but that leads to parameter explosion. Since we won't expect the model to be very good we'll just drop these columns now and go over bias and one hot encoding later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = mpg[[ 'displ', 'year', 'cyl']]\n",
    "y_1 = mpg['hwy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Next we'll split the data in a train and test set. For this we'll be using sci-kit learn.\n",
    "\n",
    "(A lot more on overfitting will follow in a later chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(x_1, y_1, test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that we're ready to create a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(x_train_1, y_train_1)\n",
    "print(model_1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This presents us with 3 coefficients. These give the influence of that parameter on the final model. They are:\n",
    "\n",
    "* Displacement: -2.475\n",
    "* Year: 0.150\n",
    "* Cylinders: -0.947\n",
    "\n",
    "(You may get different values, but they should be around the same order.)\n",
    "\n",
    "This means that displacement has a large, negative influence. Which makes sense, bigger engines use more gas and have lower miles per gallon. Year has a small positive influence, so newer cars do more miles per gallon. More cylinders also mean less miles (since they use more fuel).\n",
    "\n",
    "What would the coefficient have been if we kept cty in there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = mpg[[ 'displ', 'year', 'cyl', 'cty']]\n",
    "y_2 = mpg['hwy']\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x_2, y_2, test_size = 0.3)\n",
    "\n",
    "model_2 = LinearRegression()\n",
    "model_2.fit(x_train_2, y_train_2)\n",
    "print(model_2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The influence of the others goes down, the influence of cty is the biggest. We'll keep this model on hand.\n",
    "\n",
    "(Fun fact, if you re-run the above cell, the coefficients will change. If you rerun the cell above where we calculate the coefficients for the other model they don't change when you run it again. Why is that?\n",
    "\n",
    "[thinking-link](https://www.youtube.com/watch?v=5dlubcRwYnI)\n",
    "\n",
    "\"train_test_split\" We're splitting the data again. This is done randomly, so we get a different training-set (and thus different results) everytime we run this cell. This doesn't happen in the cell before because it only contains training, not splitting. This can be avoided by setting a random seed (which you should do to get repeatable results).)\n",
    "\n",
    "By the way, a better way of showing these coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_1.coef_, x_1.columns, columns = ['Coeff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "The models we made never saw the actual (known) values for the test-set. So how well would it be able to predict these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_1 = model_1.predict(x_test_1)\n",
    "predictions_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply printing the predicted values isn't that helpfull. Perhaps a scatterplot vs the actual values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test_1, predictions_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a perfect model these should all be on the diagonal starting bottom left and going upper right. That is not the case. Perhaps the other model is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = model_2.predict(x_test_2)\n",
    "plt.scatter(y_test_2, predictions_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I see a car that has 4 cylinders, an engine displacement of 2.0 and is built in 2024, what would the expected cty be according to the first model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_car = pd.DataFrame(data = [[2.0, 2000, 4]], columns = ['displ', 'year', 'cyl'])\n",
    "\n",
    "model_1.predict(new_car)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters on the model\n",
    "\n",
    "We noticed that the second model is better than the first, but how much better? Can this be quantified?\n",
    "\n",
    "A good way af measuring this is modeling the residuals. If there is a normal spread than our models models every relationship in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_test_1 - predictions_1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_test_2 - predictions_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look similar but if you look at the bottom axis you'll notice the first model has a much wider spread. This implies the second model is better.\n",
    "\n",
    "The bump on the right are the two-seaters, small cars with low hwy. Since we dropped the class of the cars (it was text) this relationship is not in our model.\n",
    "\n",
    "Interpreting graphs will always feel a bit random. Luckily there are numbers we can calculate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "mae_1 = metrics.mean_absolute_error(y_test_1, predictions_1)\n",
    "mse_1 = np.sqrt(metrics.mean_squared_error(y_test_1, predictions_1))\n",
    "mae_2 = metrics.mean_absolute_error(y_test_2, predictions_2)\n",
    "mse_2 = np.sqrt(metrics.mean_squared_error(y_test_2, predictions_2))\n",
    "\n",
    "mae_1, mse_1, mae_2, mse_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The errors are clearly lower for the second model, but still not very good. Which kind of makes sense, as we're trying to predict based on 235 rows (tenfold of that would be a start for a decent model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the model\n",
    "\n",
    "Let's store this model for later usage in our streamlit-app. To export a model we'll store it as pickle-file, which is derived from pickling as you do with veggies in summer to keep them edible in winter (or the next summer, or maybe even 30 years from now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../files/mpg_model.pkl', 'wb') as file: \n",
    "      \n",
    "    # A new file will be created \n",
    "    pickle.dump(model_1, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it would come down to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../files/mpg_model.pkl', 'rb') as file: \n",
    "      \n",
    "    # Call load method to deserialze \n",
    "    model_3 = pickle.load(file) \n",
    "  \n",
    "model_3.predict(new_car)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is 1kb in size. Quite the difference compared to the LLM's of [code llama](https://ai.meta.com/blog/code-llama-large-language-model-coding/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links:\n",
    "\n",
    "https://www.freecodecamp.org/news/how-to-build-and-train-linear-and-logistic-regression-ml-models-in-python/\n",
    "\n",
    "https://www.geeksforgeeks.org/how-to-use-pickle-to-save-and-load-variables-in-python/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
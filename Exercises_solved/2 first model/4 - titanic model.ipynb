{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting titanic\n",
    "\n",
    "Where we used a linear model for the MPG-dataset, we're going to use a logistic regression model for titanic. That's because when predicting the highway miles per gallon for a car we needed a number (26, for example, or 32 for another car). When entering the titanic what we want to know is whether or not we'll survive (yes/no) with nothing in between. Logistic regression will do that for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../files/titanic3.xlsx', engine='openpyxl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two columns that kind of give the solution away: boat and body. Did anyone's body got found, who survived anyway?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "boat_died = df[(df['body'].notnull()) & (df['survived'] == 1)]\n",
    "print(boat_died[['name', 'body', 'survived']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope.\n",
    "\n",
    "Show all people who were in a boat but still died."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "boat_died = df[(df['boat'].notnull()) & (df['survived'] == 0)]\n",
    "print(boat_died[['name', 'boat', 'survived']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nine. Make a graph to put this in perspective: show nr of survivors vs nr of deceased for people in a boat and for people not in a boat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "\n",
    "# Count the number of survivors and deceased for people in a boat\n",
    "boat_survived = df[(df['boat'].notnull()) & (df['survived'] == 1)]\n",
    "boat_deceased = df[(df['boat'].notnull()) & (df['survived'] == 0)]\n",
    "\n",
    "# Count the number of survivors and deceased for people not in a boat\n",
    "not_boat_survived = df[(df['boat'].isnull()) & (df['survived'] == 1)]\n",
    "not_boat_deceased = df[(df['boat'].isnull()) & (df['survived'] == 0)]\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(['Boat Survivors', 'Boat Deceased', 'Not Boat Survivors', 'Not Boat Deceased'],\n",
    "        [len(boat_survived), len(boat_deceased), len(not_boat_survived), len(not_boat_deceased)])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Category')\n",
    "plt.xticks(rotation=(360-45))\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of Survivors vs Deceased')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce a bit of a challenge, we'll drop these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['body', 'boat'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing data\n",
    "\n",
    "Running \"df.isnull()\" will give a dataframe with \"true\" for every field that is null and \"false\" for every other frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass this to a seaborn heatmap we get a nice visual overview of where the null-values are located in this dataset.\n",
    "\n",
    "By the way: Seaborn is a different library for visualisations, an alternative to matplotlib. It has much nicer plots an is a bit easier to work with sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "sns.heatmap(df.isnull(), cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualisation shows that:\n",
    "\n",
    "* Cabin is a disaster\n",
    "* Home.dest is a bit better, but not much\n",
    "* Age is dodgy but doable\n",
    "\n",
    "We'll drop Cabin and home.dest as they won't be of much help predicting anything. We'll keep age as we can expect it will be a very good predictor. (\"Women and children first!\" or the bad guy from the titanic-movie who took some random kid with him to get on a lifeboat.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "df = df.drop(['cabin', 'home.dest'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for age, we could fill it in, but how to do that is part of a later chapter. For now we'll simply drop the rows containing null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data\n",
    "\n",
    "For this dataset we'll be building a logistic regression-model which won't work with text-values. In the MPG-example we simply dropped these values, but in this case we'd lose the very valuable gender. To fix this, we'll be introducing dummies. A dummy will create a new column for every value in the column. For example, applying to sex that would lead to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['sex'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see two columns and every person on the boat is false in one and true in the other. (If gender was unknown they'd have false in both.) That also means they encode the same information, so it is better to drop one of them. That way alle the information is encoded in a single column. We can do this by using the \"drop_first\" parameter and setting it to true.\n",
    "\n",
    "As for place of embarkment, there are 3 of them (C, Q and S). If we drop one of them the other two can't predict each other anymore.\n",
    "\n",
    "Create 2 new dataframes based on the dummies for \"sex\" and \"embarked\". Drop the first fields and add the columns to the existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "sex_data = pd.get_dummies(df['sex'], drop_first = True)\n",
    "embarked_data = pd.get_dummies(df['embarked'], drop_first = True)\n",
    "df = pd.concat([df, sex_data, embarked_data], axis = 1)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally drop \"sex\", \"embarked\", \"name\" and \"ticket\" as they won't be usable (name, ticket) or needed (sex, embarked) anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "df.drop(['name', 'ticket', 'sex', 'embarked'], axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test data\n",
    "\n",
    "Next up is splitting the data into a training set and a testing set. We'll be using the scikit-learn for this.\n",
    "\n",
    "First is creating y_data, the dataframe with what we want to predict (\"survived\") and x_data (everything else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "y_data = df['survived']\n",
    "x_data = df.drop('survived', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the train_test_split-function to split the data. Put 30% of the data in the test set.\n",
    "\n",
    "We would also like to be able to reproduce our results. Use the [\"random_state\"-parameter](https://scikit-learn.org/stable/glossary.html#term-random_state) to provide an integer. Let's use 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_training_data, x_test_data, y_training_data, y_test_data = train_test_split(x_data, y_data, test_size = 0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "After splitting the data we'll start working on the model. Create a logistic regression model from the scikit-learn package. Fit the training data (x and y) into this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(x_training_data, y_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on some of the settings you may get a convergence error. [This](https://stackoverflow.com/questions/62658215/convergencewarning-lbfgs-failed-to-converge-status-1-stop-total-no-of-iter) stackoverflow-post explains the possible causes and solutions of this very well. It boils down to not having enough data (which is true), add engineered features (the names contained a title, which we dropped now but it was actually a good predictor) or pre-process our data better (fill in the age properly). But we're still building a random forecaster, so if this happens simply increase the max_iter-parameter of the model.\n",
    "\n",
    "Next, let the model predict whether the passengers in our x_test_data have survived. Store the results in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "predictions = model.predict(x_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a confusion matrix based on these results. Use the function from the scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test_data, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* True positives (upper left): survived and were predicted as survived\n",
    "* False positives (lower left): survived but were predicted as dead\n",
    "* False negatives (upper right): dead but were predicted as survived\n",
    "* True positives (lower right): dead and were predicted as dead\n",
    "\n",
    "The upper left and lower right numbers should be the highest. Our model isn't as bad as it could have been. \n",
    "\n",
    "Also print the classification report from the scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test_data, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason of generating this report is showing the numbers we'll be discussing in the next powerpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict new values\n",
    "\n",
    "Let's predict a random person using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_person_dict = {\n",
    "    'pclass': 1,\n",
    "    'age': 35,\n",
    "    'sibsp': 3, # SibSp is the number of siblings or spouse of a person onboard.\n",
    "    'parch': 2, # Similar to the SibSp, this feature contained the number of parents or children each passenger was touring with.\n",
    "    'fare': df.fare.mean(),\n",
    "    'male': False,\n",
    "    'Q': False,\n",
    "    'S': False\n",
    "}\n",
    "\n",
    "random_person = pd.DataFrame(random_person_dict, index=[0])\n",
    "\n",
    "random_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this random person, can you have the model predict survivability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(random_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray!\n",
    "\n",
    "Let's export this model into a pickle-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('../files/titanic_model.pkl', 'wb') as file: \n",
    "    # A new file will be created \n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The streamlit-app\n",
    "\n",
    "As before we'll be building a small streamlit-app to test this model. Our inputs are:\n",
    "\n",
    "- 'pclass': 1, 2 or 3\n",
    "- 'age': slider\n",
    "- 'sibsp': slider\n",
    "- 'parch': slider\n",
    "- 'fare': options at quartiles\n",
    "- 'male': gender-checkbox\n",
    "- 'Q', 'S': 'place embarked'-option\n",
    "\n",
    "Based on these you'll need to run a prediction through our (saved) model.\n",
    "\n",
    "As for fare: fare contains outliers (one person paid a lot more that all the other people). If we'll simply make a slider based on min and max values the input will always be distorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An afterthought: use the [quantile function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.quantile.html) to calculate the quantiles (0.25, 0.5, 0.75 and 1(which is max)) for fare. Return them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.fare.quantile([0.25, 0.5, 0.75, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

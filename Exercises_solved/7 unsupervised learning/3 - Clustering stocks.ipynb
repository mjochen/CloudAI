{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47233712",
   "metadata": {},
   "source": [
    "# Clustering stocks\n",
    "\n",
    "A popular application of clustering is clustering stocks. It is a way of building a balanced portfolio of stocks.\n",
    "\n",
    "[link to the original article](https://medium.com/@facujallia/stock-classification-using-k-means-clustering-8441f75363de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0148fd",
   "metadata": {},
   "source": [
    "## Gathering data\n",
    "\n",
    "We're using a somewhat different approach here. In stead of using a pre-downloaded CSV-file or a dataset from SKlearn we're scraping the data ourselves. Start by getting the [list of SP-500-companies from wikipedia](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies). Use the pandas \"read_html\" function for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc372ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lxml\n",
    "# !pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a86d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MMM', 'AOS', 'ABT', 'ABBV', 'ACN']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define the url\n",
    "sp500_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "\n",
    "# Read in the url and scrape ticker data\n",
    "data_table = pd.read_html(sp500_url)\n",
    "tickers = data_table[0]['Symbol'].values.tolist()\n",
    "tickers = [s.replace('\\n', '') for s in tickers]\n",
    "tickers = [s.replace('.', '-') for s in tickers]\n",
    "tickers = [s.replace(' ', '') for s in tickers]\n",
    "\n",
    "tickers[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e5561",
   "metadata": {},
   "source": [
    "When we have the name, let's download the closing prices for these stocks over the last year. This involves going to your local library and getting the newspapers from the last year. In these you can check the daily prices of the stocks of your choosing.\n",
    "\n",
    "Or use [yfinance](https://ranaroussi.github.io/yfinance/), a Python-library that will do all this for you.\n",
    "\n",
    "First we'll get the closing prices for Microsoft over the last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26c9985d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2024-05-01 00:00:00-04:00    391.229401\n",
       "2024-05-02 00:00:00-04:00    394.102142\n",
       "2024-05-03 00:00:00-04:00    402.839233\n",
       "2024-05-06 00:00:00-04:00    409.654633\n",
       "2024-05-07 00:00:00-04:00    405.494080\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "dat = yf.Ticker(\"MSFT\")\n",
    "close = dat.history(period=\"1d\", start=\"2024-05-01\", end=\"2025-05-01\")[\"Close\"]\n",
    "close.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7d078",
   "metadata": {},
   "source": [
    "Next we use the \"pct_change()\"-method in pandas to get not the actual prices, but the percentage they changed against the previous value. Of this we print the mean and the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7301f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column Returns\n",
    "print(close.pct_change().mean())\n",
    "\n",
    "# Define the column Volatility\n",
    "print(close.pct_change().std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50183d11",
   "metadata": {},
   "source": [
    "Let's store everything in a dictionary:\n",
    "- Percentage of returns\n",
    "- Volatility (standard deviation on previous value)\n",
    "- divindeRate\n",
    "- trailingPE\n",
    "\n",
    "The first we multiply by 252 and the second we divide by the square root of 252. This is to annualize them, based on 252 working days a year. (An we'll remove \"MSFT\" from the tickers as we already did that one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c24db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {\n",
    "    'Ticker': \"MSFT\",\n",
    "    'Returns': close.pct_change().mean() * 252,\n",
    "    'Volatility': close.pct_change().std() * (252 ** 0.5),\n",
    "    'dividendRate': dat.info['dividendRate'],\n",
    "    'trailingPE': dat.info['trailingPE']\n",
    "}\n",
    "\n",
    "dict_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7766c",
   "metadata": {},
   "source": [
    "Now for the real work: go over all 503 tickers and store the above information in a giant pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c612ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "import numpy as np\n",
    "# df = pd.DataFrame(columns=['Ticker', 'Returns', 'Volatility', 'dividendRate', 'trailingPE'])\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    dat = yf.Ticker(ticker)\n",
    "    close = dat.history(period=\"1d\", start=\"2024-05-01\", end=\"2025-05-01\")[\"Close\"]\n",
    "    dict_data = {\n",
    "        'Ticker': ticker,\n",
    "        'Returns': close.pct_change().mean() * 252,\n",
    "        'Volatility': close.pct_change().std() * (252 ** 0.5),\n",
    "        'dividendRate': dat.info['dividendRate'] if 'dividendRate' in dat.info else np.nan,\n",
    "        'trailingPE': dat.info['trailingPE'] if 'trailingPE' in dat.info else np.nan\n",
    "    }\n",
    "    data_list.append(dict_data)\n",
    "    \n",
    "df = pd.DataFrame(data_list)   \n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c92ca",
   "metadata": {},
   "source": [
    "Are there any Na-values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4167f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e60b8d",
   "metadata": {},
   "source": [
    "Some, but only in dividendRate and trailingPE. We'll ignore them for now.\n",
    "\n",
    "Now we'll export this data to a CSV so we don't have to load it everytime we want to revisit this dataset. It takes two minutes and if we had time to sit back and wait for two minutes we'd be training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../exports/PS500.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1881e023",
   "metadata": {},
   "source": [
    "The data is yours. Now analyze it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f02f3",
   "metadata": {},
   "source": [
    "## Clustering Returns and Volatility\n",
    "\n",
    "Start by re-importing the data from the CSV. If you've just loaded the data you can skip this, but if you're restarting this notebook this saves you some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../exports/PS500.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa0cba",
   "metadata": {},
   "source": [
    "Create an elbow curve to see how many clusters would be suitable. Limit yourself to Returns and Volatility, as these are roughly in the same range. (You remember that KMeans is sensitive to unscaled data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68781c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "df_rv  = df[['Returns', 'Volatility']]\n",
    "\n",
    "inertias = []\n",
    "for k in range(2,20):\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(df_rv)\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 20), inertias, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.grid(axis='x', which='both', linestyle='--', alpha=0.7)\n",
    "plt.grid(axis='y', which='both', linestyle='-', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb33d2",
   "metadata": {},
   "source": [
    "Looks like 5 is a good break-of point. Create a new model there and draw a colored scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd13b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "model_5 = KMeans(n_clusters=5, random_state=42)\n",
    "labels = model_5.fit_predict(df_rv)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df_rv['Returns'], df_rv['Volatility'], c=labels, cmap='tab10', alpha=0.7)\n",
    "plt.xlabel('Returns')\n",
    "plt.ylabel('Volatility')\n",
    "plt.title('KMeans Clustering (k=5) of S&P 500 Stocks')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d43369",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "In the scatterplot we see some obvious outliers appearing. Can we find these statiscally as well?\n",
    "\n",
    "We could use the inter quartile range. Everything outside of 1.5 the IQR (which is the value at 75% of the data minus the value at 25% of the data) times 1.5 is considered an outlier. But if we up this 3 we only get the two datapoints that are above the 0.8-mark on the graph for Volatility and the datapoint on the far right and the far left for Returns. Let's work with these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd2525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "Q1, Q3 = df_rv['Volatility'].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n",
    "\n",
    "volatility_outliers = df_rv[(df_rv['Volatility'] < Q1 - 3 * IQR) | (df_rv['Volatility'] > Q3 + 3 * IQR)]\n",
    "print(volatility_outliers)\n",
    "\n",
    "Q1, Q3 = df_rv['Returns'].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(f\"Q1: {Q1}, Q3: {Q3}, IQR: {IQR}\")\n",
    "\n",
    "# Define outliers as being 1.5*IQR below Q1 or above Q3\n",
    "returns_outliers = df_rv[(df_rv['Returns'] < Q1 - 3 * IQR) | (df_rv['Returns'] > Q3 + 3 * IQR)]\n",
    "print(returns_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5a66b0",
   "metadata": {},
   "source": [
    "Once you have identified them, remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39cfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "df_rv = df_rv.merge(volatility_outliers, how='left', indicator=True)\n",
    "df_rv = df_rv[df_rv['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "df_rv = df_rv.merge(returns_outliers, how='left', indicator=True)\n",
    "df_rv = df_rv[df_rv['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "len(df_rv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd23bb5",
   "metadata": {},
   "source": [
    "Now retrain the model and show the scatterplot again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "model_5 = KMeans(n_clusters=5, random_state=42)\n",
    "labels = model_5.fit_predict(df_rv)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df_rv['Returns'], df_rv['Volatility'], c=labels, cmap='tab10', alpha=0.7)\n",
    "plt.xlabel('Returns')\n",
    "plt.ylabel('Volatility')\n",
    "plt.title('KMeans Clustering (k=5) of S&P 500 Stocks')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e77fc7",
   "metadata": {},
   "source": [
    "You could do the elbow-plot again, but you'd see it doesn't change much, 5 remains a good option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1658b",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "There is another problem (in the last scatter plot of the previous part) though. We see the groups are all side by side, indicating that the clustering worked mainly on Returns and not on Volatility. This means that Volatility is not taken into account as much as returns.\n",
    "\n",
    "Sound like a job for scalerman!\n",
    "\n",
    "<!-- ![](../files/2025-05-21-10-00-08.png) -->\n",
    "\n",
    "<img src=\"../files/2025-05-21-10-00-08.png\" width=150 />\n",
    "\n",
    "Apply a standard scaler to our Returns and Volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "rv_scaled = scaler.fit_transform(df_rv)\n",
    "df_rv_scaled = pd.DataFrame(rv_scaled, columns=df_rv.columns)\n",
    "df_rv_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f196c0c",
   "metadata": {},
   "source": [
    "Create a new elbow plot for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "inertias = []\n",
    "for k in range(2,20):\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    model.fit(df_rv_scaled)\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 20), inertias, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.grid(axis='x', which='both', linestyle='--', alpha=0.7)\n",
    "plt.grid(axis='y', which='both', linestyle='-', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f43f9b",
   "metadata": {},
   "source": [
    "The elbow is going deeper. We could start using 4, although nothing would be wrong with 5. We'll go with 5 and decide based on the scatter plot if we're happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "model_5 = KMeans(n_clusters=4, random_state=42)\n",
    "labels = model_5.fit_predict(df_rv_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(df_rv_scaled['Returns'], df_rv_scaled['Volatility'], c=labels, cmap='plasma', alpha=0.7)\n",
    "plt.xlabel('Returns')\n",
    "plt.ylabel('Volatility')\n",
    "plt.title('KMeans Clustering (k=5) of S&P 500 Stocks')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c5958",
   "metadata": {},
   "source": [
    "Way better. Now...\n",
    "\n",
    "* Export the model and the scaler\n",
    "* Apply them to your portfolio\n",
    "* Check to see which type you should by buying to balance it across all categories\n",
    "\n",
    "You could go back to the [original article](https://medium.com/@facujallia/stock-classification-using-k-means-clustering-8441f75363de) and continue reading and trying out. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f8913f",
   "metadata": {},
   "source": [
    "# Gradient boosting\n",
    "\n",
    "We started with a decision tree and a linear regressor. That last one lost because we made the data non-linear. Next we used Bagging on the decision tree to create a random forest. That was worse, but some tuning fixed it.\n",
    "\n",
    "Now we'll be doing boosting in the same dataset, starting with gradient boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460372e",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "We exported the data before into a pickle-file. That means we can quite simply import it here [again](https://en.wikipedia.org/wiki/D%C3%A9j%C3%A0_vu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212a7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open('../exports/non_linear_data.pkl', 'rb') as file:\n",
    "    data_dict = pickle.load(file)\n",
    "\n",
    "# Display the loaded data\n",
    "\n",
    "X = data_dict[\"X\"]\n",
    "y = data_dict[\"y\"]\n",
    "X_train = data_dict[\"X_train\"]\n",
    "X_test = data_dict[\"X_test\"]\n",
    "y_train = data_dict[\"y_train\"]\n",
    "y_test = data_dict[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eef44d",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "With all the data ready in files, creating and training the model shouldn't be a problem.\n",
    "\n",
    "... or maybe there is on thing that will be annoying. Look at the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88091dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30549c6c",
   "metadata": {},
   "source": [
    "Notice anything particular?\n",
    "\n",
    "![](../files/2025-05-10-11-02-54.png)\n",
    "\n",
    "This isn't a list of values, of a list of lists with one item, the value being the one item. That means something has gone wrong before, but since we have other models depending on the data being this way it's a bad idea to start changing the data generation-process.\n",
    "\n",
    "If online there we some way to [ravel](https://numpy.org/doc/stable/reference/generated/numpy.ravel.html) this list into a clean list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2b3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "y_train.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05957d",
   "metadata": {},
   "source": [
    "Feeling generous I'll save you the trouble of creating the model yourself and simply give the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf63d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize model with warm_start=True to allow tracking\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, warm_start=True, random_state=42)\n",
    "\n",
    "# Track training and test loss\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(1, 101):  # up to 100 trees\n",
    "    gbr.n_estimators = i\n",
    "    gbr.fit(X_train, y_train.ravel())\n",
    "\n",
    "    # Predict and calculate RMSE\n",
    "    y_train_pred = gbr.predict(X_train)\n",
    "    y_test_pred = gbr.predict(X_test)\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test.ravel(), y_test_pred.ravel()))\n",
    "\n",
    "    train_losses.append(train_rmse)\n",
    "    test_losses.append(test_rmse)\n",
    "\n",
    "# Plot loss curves\n",
    "plt.plot(train_losses, label=\"Train RMSE\")\n",
    "plt.plot(test_losses, label=\"Test RMSE\")\n",
    "plt.xlabel(\"Number of Trees (Boosting Iterations)\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.title(\"Gradient Boosting Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec66df",
   "metadata": {},
   "source": [
    "This code wasn't simply creating and running the model, but creating and running the model 100 times with increasing estimators. It allowed us to plot the loss-curve.\n",
    "\n",
    "The loss-curve shows how in the beginning the RMSE is bad but get's better quite fast. After a while we see the blue and orange line growing apart again. This means the model is getting to specific and learns the train-set by heart. Overfitting, in other words.\n",
    "\n",
    "The effect isn't huge, but it's there. The difficulty is seeing the exact point where they start splitting. Maybe 45?\n",
    "\n",
    "Now create a model with 45 estimators and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fa220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "# Create and fit the Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=46, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr.fit(X_train, y_train.ravel())  # Use ravel() to flatten y_train\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Mean Squared Error: {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ced4075",
   "metadata": {},
   "source": [
    "You'll note that playing around with the number of estimators 44 makes it go up again, but 46 lowers it further. But who do we hear coming around the corner?\n",
    "\n",
    "![](../files/2025-05-10-11-34-02.png)\n",
    "\n",
    "We're doing hyperparameter tuning without using a validation set! And while tuning is not a crime, tuning without a validation set is (unless we use KFold).\n",
    "\n",
    "But why, I hear you asking. Why can't we simply continue using this amount of estimators? Because a low RMSE here is **not** the goal. The RMSE on the test-set is supposed to be an indication of the RMSE on inference, which is predicting values that didn't occur in the test or the training set, and of which we have no idea what the value of Y will be. We want to predict those with the best possible model, and the RMSE on the test-set is the only way of doing that.\n",
    "\n",
    "But if we start using the test-set in training we start cheating and start to favour models that work with our particular test-set. Doing that will make our RMSE less reliable and will therefore take away our only way of knowing how good the model will perform and, what is worse, lower the quality of the model on unseen data (to which we don't know the answer).\n",
    "\n",
    "So yes, tuning without a validation is a crime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8266d",
   "metadata": {},
   "source": [
    "## Tuning with KFold\n",
    "\n",
    "Use a KFold with 5 folds and range the estimators from 1 to 100. Keep the rmse-scores in a list and select the lowest number in that list.\n",
    "\n",
    "Note how in the code below we're not using the train/test split but we're going back to the original X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f826a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Define the range of n_estimators to test\n",
    "n_estimators_range = range(1, 101)\n",
    "mean_rmse_scores = []\n",
    "\n",
    "# Perform k-fold cross-validation for each n_estimators\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    gbr.n_estimators = n_estimators\n",
    "    # Use cross_val_score to evaluate the model using negative mean squared error\n",
    "    rmse_scores = cross_val_score(gbr, X, y.ravel(), cv=kf, scoring='neg_mean_squared_error')\n",
    "    mean_rmse_scores.append(np.sqrt(-np.mean(rmse_scores)))  # Negate to get positive RMSE\n",
    "\n",
    "# Find the best number of estimators\n",
    "best_n_estimators = n_estimators_range[np.argmin(mean_rmse_scores)]\n",
    "best_rmse = min(mean_rmse_scores)\n",
    "\n",
    "print(f'Best number of estimators: {best_n_estimators} with RMSE: {best_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d568b1",
   "metadata": {},
   "source": [
    "We have 39 now, with an RMSE of 2.115, which is a bit higher than our 2.102. Train a final model on the original train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "# Now fit the final model with the best number of estimators on the full dataset\n",
    "final_gbr = GradientBoostingRegressor(n_estimators=39, random_state=42)\n",
    "final_gbr.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_pred_gradientbooster = final_gbr.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_gradientbooster))\n",
    "print(f'Mean Squared Error: {rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98801cf9",
   "metadata": {},
   "source": [
    "Finally, we'll store the predictions of this model. (Change the name of y_pred if you used another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa509bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../exports/y_pred_gradientbooster.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred_gradientbooster, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
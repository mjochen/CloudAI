{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6ccb5a",
   "metadata": {},
   "source": [
    "# Linear regressor and decision trees\n",
    "\n",
    "A decision tree can also be used to do regression. How does it compare to a linear regression model?\n",
    "\n",
    "We'll try this out on a dataset that we generate ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8021143",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_regression(n_samples=500, n_features=1, noise=15, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9993433e",
   "metadata": {},
   "source": [
    "What does the data look like? Create a scatter plot of X vs y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cac7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "plt.scatter(X, y, color='blue', alpha=0.7)\n",
    "plt.title('Scatter Plot of X vs y')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad66b1",
   "metadata": {},
   "source": [
    "## Create the models\n",
    "\n",
    "First create a linear regression model on this data. The train and test-split has been made in the first cell. Also calculate the RMSE for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Fit a Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear = linear_regressor.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n",
    "print(f\"RMSE for Linear Regression: {rmse_linear}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac5bfb8",
   "metadata": {},
   "source": [
    "Now do the same for a decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecaee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "# Fit a Decision Tree Regressor\n",
    "tree_regressor = DecisionTreeRegressor(max_depth=3)\n",
    "tree_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = tree_regressor.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
    "print(f\"RMSE for Decision Tree: {rmse_tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f31b4f",
   "metadata": {},
   "source": [
    "You'll note the RMSE for decision trees is slightly higher. Let's plot the results next to each other to analyze them visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c16639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Linear Regression results\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "plt.scatter(X_test, y_pred_linear, color='red', label='Predicted Data (Linear Regression)')\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Decision Tree Regression results\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "plt.scatter(X_test, y_pred_tree, color='green', label='Predicted Data (Decision Tree)')\n",
    "plt.title('Decision Tree Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5563ea8c",
   "metadata": {},
   "source": [
    "Although the decision tree has only predicted straight lines, it's still not a bad prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fa78ac",
   "metadata": {},
   "source": [
    "## Non linear data\n",
    "\n",
    "Suppose we make new data, this time data that is not linear but quadratic. Which model would be best in that case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate quadratic data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "X = np.sort(5 * np.random.rand(500, 1), axis=0)  # 100 random points in the range [0, 5]\n",
    "y = X**2 + np.random.randn(500, 1) * 2  # Quadratic relationship with some noise\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa977d",
   "metadata": {},
   "source": [
    "Draw the graph again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "plt.scatter(X, y, color='blue', alpha=0.7)\n",
    "plt.title('Scatter Plot of X vs y')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d3cb3",
   "metadata": {},
   "source": [
    "Now create the same two models as before, print the RMSE's and the graphs.\n",
    "\n",
    "(This comes down to copy-pasting all three code-blocks from above here.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12aeab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "# Fit a Linear Regression model\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_linear = linear_regressor.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n",
    "print(f\"RMSE for Linear Regression: {rmse_linear}\")\n",
    "\n",
    "# Fit a Decision Tree Regressor\n",
    "tree_regressor = DecisionTreeRegressor(max_depth=3)\n",
    "tree_regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = tree_regressor.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
    "print(f\"RMSE for Decision Tree: {rmse_tree}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Linear Regression results\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "plt.scatter(X_test, y_pred_linear, color='red', label='Predicted Data (Linear Regression)')\n",
    "plt.title('Linear Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Decision Tree Regression results\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "plt.scatter(X_test, y_pred_tree, color='green', label='Predicted Data (Decision Tree)')\n",
    "plt.title('Decision Tree Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298b79d",
   "metadata": {},
   "source": [
    "As was to be expected the decision tree is much better now. So to summarise: If you're sure the data is linear, use a linear regressor. If you're not, use a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6858ac11",
   "metadata": {},
   "source": [
    "## Data export\n",
    "\n",
    "Comparing 2 models is nice, comparing more is nicer. And we can only really compare when using the same dataset, so let's export it. And to keep things fair we'll even export the train/test-split.\n",
    "\n",
    "Because we want to put multiple variables (X_train, X_test, y_train and y_test) in a single pickle-file we'll put them in a dicitionary first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'X': X,\n",
    "    'y': y,\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "# Save the data dictionary to a file\n",
    "import pickle\n",
    "with open('../exports/non_linear_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f05dc",
   "metadata": {},
   "source": [
    "And because we'll also want to do a comparison of models, let's export the predictions on our test-set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74957ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../exports/y_pred_linear.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred_linear, f)\n",
    "\n",
    "with open('../exports/y_pred_tree.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred_tree, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
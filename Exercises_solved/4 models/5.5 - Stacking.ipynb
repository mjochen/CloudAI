{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8cf0db",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "We've made 6 models, some of them better than the others. Can't we make a model that combines all these models into one big one?\n",
    "\n",
    "Yes we can, and it's called \"stacking\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f130c11",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "We exported the data before into a pickle-file. That means we can quite simply import it here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open('../exports/non_linear_data.pkl', 'rb') as file:\n",
    "    data_dict = pickle.load(file)\n",
    "\n",
    "# Display the loaded data\n",
    "\n",
    "X_train = data_dict[\"X_train\"]\n",
    "X_test = data_dict[\"X_test\"]\n",
    "y_train = data_dict[\"y_train\"]\n",
    "y_test = data_dict[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082abcf3",
   "metadata": {},
   "source": [
    "## The base models\n",
    "\n",
    "To create a stacking model we'll need to follow the following steps:\n",
    "\n",
    "Step-by-Step Implementation\n",
    "\n",
    "1) Train base models.\n",
    "1) Generate predictions from base models.\n",
    "1) Train the meta-model using the predictions from the base models.\n",
    "1) Evaluate the performance of the stacked model.\n",
    "\n",
    "We've already trained the base models and generated the prediction. These are all safely stored in pickle-files, so we'll need to import them first.\n",
    "\n",
    "Right?\n",
    "\n",
    "No. What we saved were the predictions on the test-set, not on the training-set. To train a model we'll need all that data, not just the predictions. That means we'll have to rebuild all models (as we didn't save those).\n",
    "\n",
    "But repeating is part of the learning process, so let's start with the linear regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de6881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_train, y_train)\n",
    "y_pred_lr = linear_regressor.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96928aa1",
   "metadata": {},
   "source": [
    "Next was the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_regressor = DecisionTreeRegressor(max_depth=3)\n",
    "tree_regressor.fit(X_train, y_train)\n",
    "y_pred_tree = tree_regressor.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604c0c3",
   "metadata": {},
   "source": [
    "Random forest! We found out the best parameters were:\n",
    "\n",
    "```Python\n",
    "{'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "random_forest_regressor = RandomForestRegressor(n_estimators=200, random_state=42, max_depth=5, min_samples_leaf=4, min_samples_split=10)\n",
    "random_forest_regressor.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_pred_forest = random_forest_regressor.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edaff5",
   "metadata": {},
   "source": [
    "Now the boosters, starting with the gradient booster. Try 46 estimators, not the 39 the grid-search came up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5588d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "final_gbr = GradientBoostingRegressor(n_estimators=46, random_state=42)\n",
    "final_gbr.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_pred_gradientbooster = final_gbr.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d733cf",
   "metadata": {},
   "source": [
    "And finally XGBoost. The best parameters there were:\n",
    "\n",
    "```Python\n",
    "{'colsample_bytree': 0.6, 'gamma': 0.2, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.6}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_regressor = xgb.XGBRegressor(\n",
    "    colsample_bytree=0.6,\n",
    "    gamma=0.2,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    n_estimators=50,\n",
    "    subsample=0.6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_regressor.fit(X_train, y_train.ravel())\n",
    "y_pred_xgb = xgb_regressor.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06260af2",
   "metadata": {},
   "source": [
    "## The meta-model\n",
    "\n",
    "First thing we'll need to do is create a dataset to train the model on. For this we'll paste the predicitons together. This will be our X. The actual Y-data remains, as it is what we want to predict.\n",
    "\n",
    "So paste all our y_pred-variables together. Use np.column_stack.\n",
    "\n",
    "Note: to check the meta-model we'll also need a dataset with all the predictions on the test-sets. We didn't create those yet, but the models are still in memory. Assemble a second numpy-dataset with all predictions on the testing-data.\n",
    "\n",
    "Another note: You may want to ravel the Y-predictions for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe3333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "import numpy as np\n",
    "meta_X_train = np.column_stack((y_pred_lr.ravel(), y_pred_tree, y_pred_forest, y_pred_gradientbooster, y_pred_xgb)) \n",
    "meta_X_test = np.column_stack((linear_regressor.predict(X_test), tree_regressor.predict(X_test), random_forest_regressor.predict(X_test), final_gbr.predict(X_test), xgb_regressor.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8c7c7",
   "metadata": {},
   "source": [
    "Next we'll train a linear regressor as our meta-model. Why the worst model in our test?\n",
    "\n",
    "* Simplicity: Linear regression is simple and interpretable. It can effectively capture linear relationships between the predictions of the base models and the target variable.\n",
    "* Speed: Linear regression is computationally efficient, making it quick to train and evaluate, especially when the number of base model predictions is relatively small.\n",
    "* Baseline Performance: Linear regression can serve as a good baseline model. If it performs well, it indicates that the base models are providing useful information.\n",
    "* Avoiding Overfitting: A simpler model like linear regression is less likely to overfit the predictions of the base models, especially if the base models are already complex (e.g., decision trees or ensemble methods).\n",
    "\n",
    "When to Use More Complex Meta-Models\n",
    "\n",
    "* Non-Linear Relationships: If the relationship between the predictions of the base models and the target variable is non-linear, using a more complex meta-model like a decision tree or a random forest may be beneficial.\n",
    "* Diversity of Base Models: If the base models are diverse and capture different aspects of the data, a more flexible meta-model can help combine their predictions more effectively.\n",
    "* Performance Improvement: If initial experiments show that a simple meta-model (like linear regression) does not perform well, it may be worth trying more complex models like decision trees, random forests, or even gradient boosting models.\n",
    "\n",
    "When choosing a meta-model for a stacking ensemble, several factors should be considered. First, the nature of the problem\u2014such as whether it is a regression or classification task\u2014can guide model selection, as regression tasks may be well-served by linear models, while classification tasks might benefit from more complex approaches.\n",
    "\n",
    "The complexity of the base models is also important; if the base models are already complex (e.g., ensemble methods), a simpler meta-model can help prevent overfitting, whereas simple base models may justify the use of a more complex meta-model. Additionally, the size of the dataset plays a role: smaller datasets often favor simpler models that generalize better, while larger datasets can support more complex ones.\n",
    "\n",
    "Cross-validation should be used to empirically assess the performance of different meta-models and determine which is most effective in a given stacking configuration. Ultimately, experimentation is essential\u2014trying out various meta-models and comparing their performance using relevant evaluation metrics is the best way to arrive at an optimal solution.\n",
    "\n",
    "For now, we'll try a linear regressor and a decision tree. We'll stop there and not do all five models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Fit the meta-model\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(meta_X_train, y_train)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "final_predictions = meta_model.predict(meta_X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
    "print(f'Root Mean Squared Error (Stacked Model): {rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2b8db5",
   "metadata": {},
   "source": [
    "And a decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ece463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "# Fit the meta-model using a Decision Tree\n",
    "meta_model_tree = DecisionTreeRegressor(random_state=42)\n",
    "meta_model_tree.fit(meta_X_train, y_train)\n",
    "\n",
    "# Make predictions with the meta-model\n",
    "final_predictions = meta_model_tree.predict(meta_X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\n",
    "print(f'Root Mean Squared Error (Stacked Model with Decision Tree): {rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277ead8",
   "metadata": {},
   "source": [
    "2.1 vs 2.9, Decision tree it is! Although a random forest just may be better, with the tuning and all.\n",
    "\n",
    " ![](../files/2025-05-10-13-28-53.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e23a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145947b",
   "metadata": {},
   "source": [
    "## Implementing stacking\n",
    "\n",
    "We have the base learners (linear_regressor, tree_regressor, random_forest_regressor, final_gbr, xgb_regressor) and the meta model (meta_model). Now stack'em up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20118019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Define the base learners\n",
    "base_learners = [\n",
    "    ('linear_regressor', linear_regressor),\n",
    "    ('tree_regressor', tree_regressor),\n",
    "    ('random_forest_regressor', random_forest_regressor),\n",
    "    ('gradient_boosting_regressor', final_gbr),\n",
    "    ('xgb_regressor', xgb_regressor)\n",
    "]\n",
    "\n",
    "# Create the StackingRegressor\n",
    "stacking_regressor = StackingRegressor(estimators=base_learners, final_estimator=meta_model)\n",
    "\n",
    "# Fit the stacking regressor\n",
    "stacking_regressor.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Make predictions\n",
    "stacking_predictions = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "stacking_rmse = np.sqrt(mean_squared_error(y_test, stacking_predictions))\n",
    "print(f'Root Mean Squared Error (Stacking Regressor): {stacking_rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691f621",
   "metadata": {},
   "source": [
    "We did it! We took all our models and created a model that was better than the best of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../exports/y_pred_stacking.pkl', 'wb') as f:\n",
    "    pickle.dump(stacking_predictions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
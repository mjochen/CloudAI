{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a6b992",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "\n",
    "We've used a decision tree before, but isn't a random forest supposed to be a better version of the decision tree? Let's try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2418e10",
   "metadata": {},
   "source": [
    "## Data import\n",
    "\n",
    "We exported the data before into a pickle-file. That means we can quite simply import it here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8973d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open('../../exports/non_linear_data.pkl', 'rb') as file:\n",
    "    data_dict = pickle.load(file)\n",
    "\n",
    "# Display the loaded data\n",
    "\n",
    "X_train = data_dict[\"X_train\"]\n",
    "X_test = data_dict[\"X_test\"]\n",
    "y_train = data_dict[\"y_train\"]\n",
    "y_test = data_dict[\"y_test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabba54a",
   "metadata": {},
   "source": [
    "## Create model\n",
    "\n",
    "With all the data ready in files, creating and training the model shouldn't be a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Create and fit the Random Forest Regressor\n",
    "random_forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "random_forest_regressor.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_pred_forest = random_forest_regressor.predict(X_test)\n",
    "\n",
    "rmse_forest = np.sqrt(mean_squared_error(y_test, y_pred_forest))\n",
    "print(f\"RMSE for Random forest: {rmse_forest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31116f3",
   "metadata": {},
   "source": [
    "We get an RMSE of 2.52, which is worse than the 2.26 that a decision tree got. How do they compare?\n",
    "\n",
    "First we'll import the y-predictions of the decision tree. This way we can compare the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../exports/y_pred_tree.pkl', 'rb') as file:\n",
    "    y_pred_tree = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a59692b",
   "metadata": {},
   "source": [
    "And now you're ready to copy, paste and adapt the code for the graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa833f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "import matplotlib.pyplot as plt\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Linear Regression results\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "plt.scatter(X_test, y_pred_tree, color='red', label='Predicted Data (Decision Tree)')\n",
    "plt.title('Decision Tree Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Decision Tree Regression results\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "plt.scatter(X_test, y_pred_forest, color='green', label='Predicted Data (Random Forest)')\n",
    "plt.title('Random Forest Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b606b3",
   "metadata": {},
   "source": [
    "We see the random forest predictions aren't on a line like the decision tree. That is because it takes the average of many different trees as the output value.\n",
    "\n",
    "It's still less good, so what could be the cause there? The problem is we didn't do any tuning on our model. Because:\n",
    "\n",
    "![](../../../files/2025-05-09-19-20-34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17be174",
   "metadata": {},
   "source": [
    "## Random forest tuning\n",
    "\n",
    "Our 500 datapoints are not a big dataset. That's why when doing hyperparameter tuning a kfold-algorithm is a good idea. Implement it with the following parametergrid:\n",
    "\n",
    "```Python\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [None, 5, 10, 15],  # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]     # Minimum samples required to be at a leaf node\n",
    "}\n",
    "```\n",
    "\n",
    "Note how we now have 4-dimensional grid with 3x4x3x3 fields, totalling 108 models. Every one of these 108 models is then tested against 5 folds, leading 504 models to train. Best to give it some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37fd959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [None, 5, 10, 15],  # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4]     # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid,\n",
    "                           scoring='neg_mean_squared_error', cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_  # Negate because we used neg_mean_squared_error\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'Best Cross-Validated MSE: {best_score:.2f}')\n",
    "\n",
    "# Make predictions with the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error on the test set\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f'Mean Squared Error (Random Forest with Best Params): {mse_rf:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae7c9d",
   "metadata": {},
   "source": [
    "About 16 seconds on my PC. Not too bad.\n",
    "\n",
    "Retrain the model with the parameters we just got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141dff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create and fit the Random Forest Regressor\n",
    "random_forest_regressor = RandomForestRegressor(n_estimators=200, random_state=42, max_depth=5, min_samples_leaf=4, min_samples_split=10)\n",
    "random_forest_regressor.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_pred_forest = random_forest_regressor.predict(X_test)\n",
    "\n",
    "rmse_forest = np.sqrt(mean_squared_error(y_test, y_pred_forest))\n",
    "print(f\"RMSE for Random forest: {rmse_forest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d1015d",
   "metadata": {},
   "source": [
    "RMSE now dips significantly below the RMSE of a decision tree. This should show in the data as well. Recreate the graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Linear Regression results\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "plt.scatter(X_test, y_pred_tree, color='red', label='Predicted Data (Decision Tree)')\n",
    "plt.title('Decision Tree Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Decision Tree Regression results\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual Data')\n",
    "plt.scatter(X_test, y_pred_forest, color='green', label='Predicted Data (Random Forest)')\n",
    "plt.title('Random Forest Regression')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bdfc44",
   "metadata": {},
   "source": [
    "The data now resembles the original idea, a quadratic equation, very well. The offset we're seeing is the noise that was (deliberatly) inserted into the model. That also means our model succeeded in filtering out that noise.\n",
    "\n",
    "Let's remember the number to beat: 2.084."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c707815",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../exports/y_pred_randomforest.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred_forest, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
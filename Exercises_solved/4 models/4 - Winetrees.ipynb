{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6911869",
   "metadata": {},
   "source": [
    "# Winetrees\n",
    "\n",
    "This notebook works with the wine-dataset and hopes to show you the benefits of k-fold and gridsearch.\n",
    "\n",
    "Let's begin by importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde31c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X, y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589563b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert X to DataFrame\n",
    "df_wine = pd.DataFrame(X)\n",
    "\n",
    "# Add y as a new column named \"type\"\n",
    "df_wine['type'] = y\n",
    "\n",
    "df_wine.head()\n",
    "\n",
    "df_wine.to_csv('wine.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80a1ca",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Get the info and the size of the wine-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Number of classes: {len(set(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ec944",
   "metadata": {},
   "source": [
    "Now some data-exploring. Create three different graphs about your dataset. Let's say:\n",
    "\n",
    "- A histogram of the first feature\n",
    "- All boxplots\n",
    "- Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Histogram of the first feature\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(X[:, 0], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of the First Feature')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of all features\n",
    "for i in range(X.shape[1]):\n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.boxplot(X[:, i], vert=False, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "    plt.title(f'Boxplot of Feature {i + 1}')\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Feature Index')\n",
    "    plt.show()\n",
    "\n",
    "# Heatmap of the correlation matrix\n",
    "correlation_matrix = np.corrcoef(X, rowvar=False)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', cbar=True)\n",
    "plt.title('Heatmap of Feature Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc97e20",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "Normally we'd now split the dataset in a train and test set, but the dataset is very small. Make a decision tree of max 3 branches deep, using the random state of 42. Also setup KFold with 5 splits and shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3194f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "kf = KFold(n_splits=8, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b8f8c7",
   "metadata": {},
   "source": [
    "Now go over all the splits in the KFold-object you just made and store the accuracies in a list. Print the list and the mean of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac43f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and score\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    scores.append(acc)\n",
    "\n",
    "# Output results\n",
    "print(\"Fold accuracies:\", scores)\n",
    "print(\"Average accuracy:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c77ca",
   "metadata": {},
   "source": [
    "Now play around with the tree depth and number of folds a bit. The idea is to get the highest possible average accuracy.\n",
    "\n",
    "But wait, you may think, why not take the fold with the highest accuracy? Because this every fold is only trained on part of the dataset. If you were to take that particular model you'd be overfitting on that particular fold.\n",
    "\n",
    "That's why we are playing around with the numbers. What we want to achieve is **hyper parameter tuning**, and the hyper parameter is the depth of the tree. Too high and we're overfitting, not high enough and we'll have a bad fit. By trying out all the numbers by hand we can see where the number is best.\n",
    "\n",
    "And the k-fold helps us doing this without splitting the data in three parts (train, test, validation), because we don't have enough data for that (only 176 rows).\n",
    "\n",
    "Another thing nagging in your mind right now: Why am I changing numbers manually and test them? Can't we automate that? And yes, we can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5e978e",
   "metadata": {},
   "source": [
    "## Gridsearch\n",
    "\n",
    "Gridsearch is a way to automate hyper parameter tuning. We can input a number of parameters in which we want to test a model.\n",
    "\n",
    "We'll be using it to tune another Decision tree parameter: min_samples_split. This parameter means \"Don't split a node unless it has at least this many samples.\". If we allow the model to keep on splitting nodes we'll end up with a lot of leaves in our tree, which means we're probably overfitting. Keeping it to big will lead to underfitting.\n",
    "\n",
    "We'll try max_depths of 2, 3, 4, 5, 6 and min_samples_splits of 2, 4, 6, 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 4, 6, 10]\n",
    "}\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit to the full dataset (CV will split internally)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Output\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validated accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367d795",
   "metadata": {},
   "source": [
    "What happened now is that the grid_search made a grid 20 (=5x4) squares, combining all max_depths and all min_samples_splits. From this grid it deduced that a max_depth of 4 and a min_samples_split of 6 will work best.\n",
    "\n",
    "Try to show the full grid as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110def31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "for mean, std, params in zip(\n",
    "    grid_search.cv_results_['mean_test_score'],\n",
    "    grid_search.cv_results_['std_test_score'],\n",
    "    grid_search.cv_results_['params']\n",
    "):\n",
    "    print(f\"{params} => Accuracy: {mean:.3f} (+/- {std:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf77f4",
   "metadata": {},
   "source": [
    "In a heatmap even?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DELETE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "heatmap_data = results.pivot(\n",
    "    index=\"param_max_depth\",\n",
    "    columns=\"param_min_samples_split\",\n",
    "    values=\"mean_test_score\"\n",
    ")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".3f\", cmap=\"viridis\")\n",
    "plt.title(\"GridSearchCV Mean Accuracy\")\n",
    "plt.ylabel(\"max_depth\")\n",
    "plt.xlabel(\"min_samples_split\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
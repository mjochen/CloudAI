{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94ae4f8",
   "metadata": {},
   "source": [
    "# An overview of machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3bb59",
   "metadata": {},
   "source": [
    "## Tree-Based Models\n",
    "\n",
    "| Model                 | Description                                                                                                  |\n",
    "| --------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
    "| **Decision Trees**    | Simple, interpretable models that split data based on feature thresholds. Prone to overfitting.              |\n",
    "| **Random Forest**     | An ensemble of decision trees trained on random subsets of data and features (bagging). Reduces overfitting. |\n",
    "| **Gradient Boosting** | Sequentially builds trees that correct errors of previous ones. Slower, but more accurate.                   |\n",
    "| **XGBoost**           | Optimized implementation of gradient boosting. Fast, regularized, and widely used in competitions.           |\n",
    "| **LightGBM**          | Gradient boosting library focused on speed and memory efficiency. Good for large datasets.                   |\n",
    "| **CatBoost**          | Gradient boosting that handles categorical features natively. Great out-of-the-box performance.              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731d92d",
   "metadata": {},
   "source": [
    "## Linear models\n",
    "\n",
    "| Model                      | Description                                                                     |\n",
    "| -------------------------- | ------------------------------------------------------------------------------- |\n",
    "| **Linear Regression**      | Predicts continuous values using a weighted sum of features. Easy to interpret. |\n",
    "| **Logistic Regression**    | For binary classification. Outputs probability; still linear in nature.         |\n",
    "| **Ridge/Lasso Regression** | Linear regression with regularization (L2 or L1) to reduce overfitting.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff6ef6",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "| Model                                    | Description                                                            |\n",
    "| ---------------------------------------- | ---------------------------------------------------------------------- |\n",
    "| **Multilayer Perceptron (MLP)**          | Fully connected feedforward neural network. Good for tabular data.     |\n",
    "| **Convolutional Neural Networks (CNNs)** | Designed for image data. Capture spatial patterns using filters.       |\n",
    "| **Recurrent Neural Networks (RNNs)**     | Designed for sequential data (e.g. time series, text).                 |\n",
    "| **Transformers**                         | Modern neural architecture for sequences, especially effective in NLP. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b97ae",
   "metadata": {},
   "source": [
    "## Others\n",
    "\n",
    "| Type| Model                        | Description                                                                                      |\n",
    "| -----| ---------------------------- | ------------------------------------------------------------------------------------------------ |\n",
    "|Support Vector Machines (SVM)| **SVM Classifier/Regressor** | Finds a hyperplane that maximizes margin between classes. Works well with high-dimensional data. |\n",
    "|Instance based methods| **k-Nearest Neighbors (k-NN)** | Predicts by looking at the closest k data points in the training set. No training time, slow prediction. |\n",
    "|Naive Bayes| **Gaussian/Bernoulli/Multinomial Naive Bayes** | Probabilistic classifier based on Bayes' theorem. Assumes feature independence. Surprisingly effective for text. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc3481d",
   "metadata": {},
   "source": [
    "## Clustering (unsupervised)\n",
    "\n",
    "(These come back in a later chapter)\n",
    "\n",
    "| Model                       | Description                                                            |\n",
    "| --------------------------- | ---------------------------------------------------------------------- |\n",
    "| **K-Means**                 | Groups data into k clusters by minimizing within-cluster distance.     |\n",
    "| **Hierarchical Clustering** | Builds a tree of clusters via bottom-up or top-down approach.          |\n",
    "| **DBSCAN**                  | Density-based clustering \u2014 detects clusters of varying shape and size. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a4500",
   "metadata": {},
   "source": [
    "## Time series models\n",
    "\n",
    "(These also get their very own chapter.)\n",
    "\n",
    "| Model                                                                 | Description                                                                                                      |\n",
    "| --------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |\n",
    "| **ARIMA (AutoRegressive Integrated Moving Average)**                  | Classic statistical model. Good for univariate forecasting when data is stationary or can be differenced.        |\n",
    "| **SARIMA (Seasonal ARIMA)**                                           | Extension of ARIMA to handle **seasonality** explicitly.                                                         |\n",
    "| **Exponential Smoothing (ETS)**                                       | Models level, trend, and seasonality with smoothing coefficients. Good for simple, interpretable forecasts.      |\n",
    "| **Prophet (by Facebook)**                                             | Decomposes time series into trend + seasonality + holidays. Easy to tune, works well on business data.           |\n",
    "| **Vector AutoRegression (VAR)**                                       | Generalizes ARIMA to multivariate time series \u2014 models relationships between multiple time-dependent variables.  |\n",
    "| **LSTM (Long Short-Term Memory networks)**                            | A type of RNN that can learn long-term dependencies. Powerful for sequential or time series data.                |\n",
    "| **Temporal Convolutional Networks (TCNs)**                            | Use 1D convolutions over time series. Often faster and more stable than RNNs.                                    |\n",
    "| **Transformer-based models (e.g. Time Series Transformer, Informer)** | Recent advances in time series using attention mechanisms; good for long sequences and multivariate forecasting. |\n",
    "| **XGBoost, LightGBM for time series**                                 | Tree-based models can also be used for time series **if you engineer lag, rolling, and time features manually**. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
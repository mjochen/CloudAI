{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike highways - revisit manually\n",
    "\n",
    "After AWS and PyCaret it's still nice to do a bit of predicting ourselves. Let's load the same dataset (again) and see of any of the models we saw will be able to provide a good prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('files/bike_counters_data/Measured data-nl-Geel_FMN GV 21 Geel.csv')\n",
    "\n",
    "df[\"date_time\"] = df[\"Datum\"] + \" \" + df[\"Tijd\"]\n",
    "df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "df = df.set_index(\"date_time\")\n",
    "df = df[[\"Aantal fietsers\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, group by month and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_month = df.loc[ df[\"Aantal fietsers\"] >= 30 ].resample('ME').sum().reset_index().set_index(['date_time'])\n",
    "df_time_month.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the autocorrection on this dataset. This should show us any seasonality that is in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "\n",
    "pd.plotting.autocorrelation_plot(df_time_month['Aantal fietsers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not as obvious as the example dataset, but there is a definite spike at 12 and 24 (months). And this is significant because we only have three years worth of data.\n",
    "\n",
    "What if we group the data by day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_day = df.loc[ df[\"Aantal fietsers\"] >= 30 ].resample('D').sum().reset_index().set_index(['date_time'])\n",
    "# df_time_day.plot()\n",
    "pd.plotting.autocorrelation_plot(df_time_day['Aantal fietsers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same spike at 365 and 730! But not much higher, so we could simply keep on working with the monthly data..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arima\n",
    "\n",
    "* [Try this one](https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/)\n",
    "* [Or this one](https://www.geeksforgeeks.org/python-arima-model-for-time-series-forecasting/)\n",
    "\n",
    "Both require the statsmodel package to check for stationarity. You can install it using pip. If you use the bottom one you'll also need pmdarima. The solution uses the bottom one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install statsmodels\n",
    "%pip install pmdarima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the model. Easiest way to go about it is to reload the excel file, set the date as index, drop all columns besides \"Aantal fietsers\" and resample as months.\n",
    "\n",
    "Then do a seasonal decompose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "df = pd.read_csv('files/bike_counters_data/Measured data-nl-Geel_FMN GV 21 Geel.csv')[ [\"Datum\", \"Aantal fietsers\"] ]\n",
    "df = df.loc[ df['Aantal fietsers'] > 1 ]\n",
    "df[\"Datum\"] = pd.to_datetime(df[\"Datum\"])\n",
    "df1 = df.resample('M', on='Datum').sum()\n",
    "\n",
    "result = seasonal_decompose(df1['Aantal fietsers'],  model ='additive')\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try to fit an Arima model on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\") \n",
    "  \n",
    "# Fit auto_arima function to AirPassengers dataset \n",
    "stepwise_fit = auto_arima(df1['Aantal fietsers'], start_p = 1, start_q = 1, \n",
    "                          max_p = 3, max_q = 3, m = 12, \n",
    "                          start_P = 0, seasonal = True, \n",
    "                          d = None, D = 1, trace = True, \n",
    "                          error_action ='ignore',   # we don't want to know if an order does not work \n",
    "                          suppress_warnings = True,  # we don't want convergence warnings \n",
    "                          stepwise = True)           # set to stepwise \n",
    "  \n",
    "# To print the summary \n",
    "stepwise_fit.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model using the parameters you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "\n",
    "# Split data into train / test sets \n",
    "train = df1.iloc[:len(df1)-12] \n",
    "test = df1.iloc[len(df1)-12:] # set one year(12 months) for testing \n",
    "  \n",
    "# Fit a SARIMAX(1, 0, 0)x(0, 1, [1], 12) on the training set \n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX \n",
    "  \n",
    "model = SARIMAX(train['Aantal fietsers'],  \n",
    "                order = (1, 0, 0),  \n",
    "                seasonal_order =(0, 1, 1, 12)) \n",
    "  \n",
    "result = model.fit() \n",
    "result.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "\n",
    "start = len(train) \n",
    "end = len(train) + len(test) - 1\n",
    "  \n",
    "# Predictions for one-year against the test set \n",
    "predictions = result.predict(start, end, \n",
    "                             typ = 'levels').rename(\"Predictions\") \n",
    "  \n",
    "# plot predictions and actual values \n",
    "predictions.plot(legend = True) \n",
    "test['Aantal fietsers'].plot(legend = True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what are the MSE and RMSE of our model? We know by the graph it won't be great, but still doable, no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load specific evaluation tools \n",
    "from sklearn.metrics import mean_squared_error \n",
    "from statsmodels.tools.eval_measures import rmse \n",
    "  \n",
    "# Calculate root mean squared error \n",
    "print(rmse(test[\"Aantal fietsers\"], predictions))\n",
    "  \n",
    "# Calculate mean squared error \n",
    "print(mean_squared_error(test[\"Aantal fietsers\"], predictions) )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

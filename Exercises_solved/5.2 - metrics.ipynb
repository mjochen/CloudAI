{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Let's experiment with the metrics. In the following exercises, the confusion matrices are given and you need to decide which one to use.\n",
    "\n",
    "We'll start be defining a class for you to use, as it's easier than a dict in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "TP: 107\t| FP: 23 \n",
      "FN: 69\t| TN: 42\n"
     ]
    }
   ],
   "source": [
    "# Some preliminary code\n",
    "class Confusion_matrix():\n",
    "\n",
    "    def __init__(self, tp, tn, fp, fn):\n",
    "        self.tp = tp\n",
    "        self.tn = tn\n",
    "        self.fp = fp\n",
    "        self.fn = fn\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Confusion matrix: \\n\" \\\n",
    "                f\"TP: {self.tp}\\t| FP: {self.fp} \\n\" \\\n",
    "                f\"FN: {self.fn}\\t| TN: {self.tn}\"\n",
    "\n",
    "cats = Confusion_matrix(107, 42, 23, 69)\n",
    "print(cats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Consider two classification models, Model A and Model B, trained to detect fraudulent transactions. You have their respective confusion matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A: \n",
      "Confusion matrix: \n",
      "TP: 150\t| FP: 50 \n",
      "FN: 20\t| TN: 280\n",
      "\n",
      "Model B: \n",
      "Confusion matrix: \n",
      "TP: 200\t| FP: 40 \n",
      "FN: 40\t| TN: 260\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "models[\"A\"] = Confusion_matrix(150, 280, 50, 20)\n",
    "models[\"B\"] = Confusion_matrix(200, 260, 40, 40)\n",
    "\n",
    "print(f\"Model A: \\n{models['A']}\")\n",
    "print(f\"\\nModel B: \\n{models['B']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the accuracy for both models and determine which model has a higher accuracy.\n",
    "1. Calculate the precision for both models and identify which model has higher precision.\n",
    "1. Calculate the sensitivity for both models and identify which model has higher sensitivity.\n",
    "1. Calculate the specificity for both models and identify which model has higher specificity.\n",
    "1. Calculate the F1 score for both models and determine which model has a higher F1 score.\n",
    "\n",
    "Tip: look at [the eval-function](https://realpython.com/python-eval-function/) to keep the copy-pasting to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A: \n",
      "Confusion matrix: \n",
      "TP: 150\t| FP: 50 \n",
      "FN: 20\t| TN: 280\n",
      "\n",
      "Model B: \n",
      "Confusion matrix: \n",
      "TP: 200\t| FP: 40 \n",
      "FN: 40\t| TN: 260\n",
      "\n",
      "accuracy\t0.86\t0.852\t\n",
      "precision\t0.75\t0.833\t\n",
      "sensitivity\t0.882\t0.833\t\n",
      "specificity\t0.848\t0.867\t\n",
      "f1_score\t0.811\t0.833\t\n"
     ]
    }
   ],
   "source": [
    "#DELETE\n",
    "def accuracy(confusion_matrix):\n",
    "    return (confusion_matrix.tp + confusion_matrix.tn) / (confusion_matrix.tp + confusion_matrix.tn + confusion_matrix.fp + confusion_matrix.fn)\n",
    "\n",
    "def precision(confusion_matrix):\n",
    "    return confusion_matrix.tp / (confusion_matrix.tp + confusion_matrix.fp)\n",
    "\n",
    "def sensitivity(confusion_matrix):\n",
    "    return confusion_matrix.tp / (confusion_matrix.tp + confusion_matrix.fn)\n",
    "\n",
    "def specificity(confusion_matrix):\n",
    "    return confusion_matrix.tn / (confusion_matrix.tn + confusion_matrix.fp)\n",
    "\n",
    "def f1_score(confusion_matrix):\n",
    "    return 2 * (precision(confusion_matrix) * sensitivity(confusion_matrix)) / (precision(confusion_matrix) + sensitivity(confusion_matrix))\n",
    "\n",
    "metrics = [\"accuracy\",\"precision\",\"sensitivity\",\"specificity\",\"f1_score\"]\n",
    "\n",
    "print(f\"Model A: \\n{models['A']}\\n\")\n",
    "print(f\"Model B: \\n{models['B']}\\n\")\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric, end=\"\\t\")\n",
    "    for model in models.values():\n",
    "        print(round(eval(metric)(model),3), end=\"\\t\")\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have numbers, so you can make the following decisions easily. Remember the model was about detecting fraude in transactions.\n",
    "\n",
    "1. You are a bank and legally need to have this model in place, but really you don't want to inconvenience your clients to much. Better let some bad ones get through than to annoy our good customers!\n",
    "1. You are the government and are using this model to weed out the obviously ok transactions. Anything tagged fraudulent here will be investigated and if it is then discovered that it was fine that is no problem, but you really don't want any tax dodgers to get through!\n",
    "1. You are a student who is graded on the overall performance of this model. Try to find the best performance, balanced between false positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "\n",
    "# 1. Focus on low false positives, i.e. high specificity. Model B;\n",
    "# 2. Focus on low false negatives, i.e. high sensitivity. Model A;\n",
    "# 3. Focus a good model, high accuracy. Model A;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Consider two classification models, Model X and Model Y, trained to diagnose a specific medical condition. You have their respective confusion matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "models[\"X\"] = Confusion_matrix(100, 270, 10, 20)\n",
    "models[\"Y\"] = Confusion_matrix(90, 260, 20, 10)\n",
    "\n",
    "print(f\"Model X: \\n{models['X']}\")\n",
    "print(f\"\\nModel Y: \\n{models['Y']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the accuracy for both models and determine which model has a higher accuracy.\n",
    "1. Calculate the precision for both models and identify which model has higher precision.\n",
    "1. Calculate the sensitivity for both models and identify which model has higher sensitivity.\n",
    "1. Calculate the specificity for both models and identify which model has higher specificity.\n",
    "1. Calculate the F1 score for both models and determine which model has a higher F1 score.\n",
    "\n",
    "Tip: don't rewrite the functions you made for exercises 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model X: \\n{models['X']}\\n\")\n",
    "print(f\"Model Y: \\n{models['Y']}\\n\")\n",
    "\n",
    "for metric in metrics:\n",
    "    print(metric, end=\"\\t\")\n",
    "    for model in models.values():\n",
    "        print(round(eval(metric)(model),3), end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember this is a model diagnosing a certain medical condition. Write a scenario for each metric when you would want it maximized.\n",
    "\n",
    "Inspiration for scenario's:\n",
    "* A disease that can only be cured when detected fast\n",
    "* A disease for which the cure can be dangerous\n",
    "* An insurance-company looking to minimize cost\n",
    "* A pharmaceutical company looking to boost the numbers on an illness to secure more funding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "\n",
    "# Accuracy is an overall metric, it does not tell us how well the model performs on each class. Both are comparable, so no scenario required here.\n",
    "\n",
    "# Precision: False positives are a problem, that means people who don't have the illness but are diagnosed with it.\n",
    "# You are insurance-company looking to minimize cost. Sick people need to be treated, yes, but healthy people need to be at work paying their insurance. Some sick people may be at work to, that is ok for us.\n",
    "\n",
    "# Sensitivity: False negatives are a problem, that means people who have the illness but are not diagnosed with it.\n",
    "# You are a doctor looking to minimize risk. You want to treat all sick people, even if that means treating some healthy people too.\n",
    "\n",
    "# specificity: Correctly identifying negative instances is crucial, which means people who don't have the illness and are not diagnosed with it.\n",
    "# The difference with precision is that we are not looking to minimize false positives, we are looking to maximize true negatives.\n",
    "# You are the same insurance company but got some bad reviews on the internet. You focus now still on keeping healthy people at work, but also keep in mind that sick people shouldn't be working.\n",
    "\n",
    "# f1_score: You want to balance precision and sensitivity.\n",
    "# You are a doctor looking to minimize risk, but you also want to minimize cost. You want to treat all sick people, but you also want to minimize the amount of healthy people you treat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

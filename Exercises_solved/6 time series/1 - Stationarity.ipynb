{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc404e9f",
   "metadata": {},
   "source": [
    "# Stationarity\n",
    "\n",
    "The notebook from [this youtube video](https://www.youtube.com/watch?v=621MSxpYv60). He uses a [dataset](https://www.kaggle.com/datasets/ashfakyeafi/air-passenger-data-for-time-series-analysis/) that is often used for this exact purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_air = pd.read_csv(\"../files/AirPassengers.csv\")\n",
    "df_air.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13163074",
   "metadata": {},
   "source": [
    "What we see is that not only is the data going up, but the fluctuations are also increasing. This means it is not stationary because:\n",
    "\n",
    "- There is a clear increasing trend\n",
    "- There is a clear increasing variance\n",
    "\n",
    "First we apply differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air[\"Passenger_Diff\"] = df_air[\"#Passengers\"].diff()\n",
    "df_air[\"Passenger_Diff\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bbe8ad",
   "metadata": {},
   "source": [
    "This completely removes the trend. That means the mean is now constant. The variance however is still unstable. This can be helped by using a log transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8a49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_air[\"Passenger_Log\"] = np.log(df_air[\"#Passengers\"])\n",
    "df_air[\"Passenger_Log\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6c972",
   "metadata": {},
   "source": [
    "Now the trend is back, but the variance is constant. This means that the variance was actually exponential in our original dataset. What is we applied them both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37bfcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air[\"Passenger_Diff_Log\"] = df_air[\"Passenger_Log\"].diff()\n",
    "df_air[\"Passenger_Diff_Log\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d32ae87",
   "metadata": {},
   "source": [
    "In the video a statistical test is now used to confirm that our data is stationary, but that would take us to far. The end of the video is though.\n",
    "\n",
    "Why are we doing this? Aren't we deleting interesting information from the data?\n",
    "\n",
    "No.\n",
    "\n",
    "Because every data point has it's own mean and variance, it belongs to a different distribution. This was not the case when predicting non-time-series data, for example: when predicting ice cream sales as a function of temperature (warm: more ice creams sold) both the temperatures and the ice creams have a mean. These averages never change! When we look at ice cream sales as a function of time (ice cream sales going up from January until July) there is no common mean anymore, and therefore they don't have a common distribution.\n",
    "\n",
    "This is the point where the youtube-video stops. But we could go further. Do some actual predictions on this dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b90ea",
   "metadata": {},
   "source": [
    "## Continue with manually transformed data\n",
    "\n",
    "Egor Howell (the youtuber) did all the hard work of differencing and log transforming for us. Training a model, running the test set and untransforming the predicitions shouldn't be too much work.\n",
    "\n",
    "Or should it?\n",
    "\n",
    "The first step is changing the index of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572489f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3f206",
   "metadata": {},
   "source": [
    "We have a \"month\"-column, but it's not a date-time column and it's not the index of the dataframe either. We need to fix this before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d2455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date range\n",
    "date_range = pd.date_range(start=\"1949-01\", periods=len(df_air), freq=\"MS\")\n",
    "\n",
    "# Assign as index\n",
    "df_air.index = date_range\n",
    "df_air.index.name = \"Month\"\n",
    "df_air.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be25ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# 1. LOAD DATA\n",
    "# (Assuming df_air is already prepared and indexed by date)\n",
    "# Apply transformations (as you did)\n",
    "df_air[\"Passenger_Log\"] = np.log(df_air[\"#Passengers\"])\n",
    "df_air[\"Passenger_Diff_Log\"] = df_air[\"Passenger_Log\"].diff()\n",
    "\n",
    "# Drop the first NaN from differencing\n",
    "df_diff = df_air[\"Passenger_Diff_Log\"].dropna()\n",
    "\n",
    "# 2. SPLIT TRAIN / TEST (e.g. 80/20 split)\n",
    "split_point = int(len(df_diff) * 0.8)\n",
    "train, test = df_diff.iloc[:split_point], df_diff.iloc[split_point:]\n",
    "\n",
    "# 3. TRAIN ARIMA MODEL (no need for differencing again)\n",
    "model = ARIMA(train, order=(2, 0, 2))  # no differencing because you already did it\n",
    "model_fit = model.fit()\n",
    "\n",
    "# 4. FORECAST\n",
    "n_periods = len(test)\n",
    "forecast_diff_log = model_fit.forecast(steps=n_periods)\n",
    "\n",
    "# 5. UNDO DIFFERENCING\n",
    "# We need the last known log value from training data\n",
    "last_log_value = df_air[\"Passenger_Log\"].iloc[split_point]\n",
    "\n",
    "# Reconstruct log values from differenced forecast\n",
    "forecast_log = forecast_diff_log.cumsum() + last_log_value\n",
    "\n",
    "# 6. UNDO LOG TRANSFORMATION\n",
    "forecast = np.exp(forecast_log)\n",
    "\n",
    "# 7. COMPARE TO ACTUALS\n",
    "actual = df_air[\"#Passengers\"].iloc[split_point + 1: split_point + 1 + n_periods]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(actual.index, actual.values, label=\"Actual\")\n",
    "plt.plot(actual.index, forecast.values, label=\"Forecast\", linestyle='--')\n",
    "plt.title(\"Forecast vs Actual Passengers\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Passengers\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d1a76",
   "metadata": {},
   "source": [
    "## Continue with auto-arima\n",
    "\n",
    "Auto-arima is supposed to do all this for us. But there is a problem. Auto-arima is part of [pmdarima](https://github.com/alkaline-ml/pmdarima) and that repo has some problems:\n",
    "\n",
    "* The last stable release was: 2.0.4 (mid 2023).\n",
    "* The GitHub repo has:\n",
    "    * Open issues and pull requests\n",
    "    * Very few responses or merges\n",
    "    * No sign of adapting to newer libraries (e.g., numpy 2.x, scikit-learn >=1.6)\n",
    "\n",
    "* In short: it's useful but aging, and compatibility issues are becoming more common.\n",
    "    * Still uses outdated scikit-learn APIs\n",
    "    * Lags behind in adapting to numpy and pandas updates\n",
    "    * Sparse documentation and long-standing bugs\n",
    "    * Seasonal support (SARIMA) is limited to stepwise search and not very robust\n",
    "\n",
    "The recommended alternative is using statsmodels, but that doesn't have an auto-arima. Anothet option is going for [statsforecast](https://github.com/Nixtla/statsforecast), which is actively maintained.\n",
    "\n",
    "You could try it out, but be prepared to uninstall your numpy (which is at version 2.X) and install an older version. There will also be a ton of warnings because auto-arima is using deprecated features from sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install statsforecast\n",
    "# !pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1295fa",
   "metadata": {},
   "source": [
    "Let's train the auto-arima model. We're using the last 12 months as test-data. We're also inputting a season-length of 12, which is something we can calculate using auto-correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b26ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA\n",
    "\n",
    "# Load and prepare the AirPassengers dataset\n",
    "df = pd.read_csv('../files/AirPassengers.csv')\n",
    "df['ds'] = pd.to_datetime(df['Month'])\n",
    "df['y'] = df['#Passengers']\n",
    "df['unique_id'] = 'air'\n",
    "\n",
    "# Keep only required columns\n",
    "df = df[['unique_id', 'ds', 'y']]\n",
    "\n",
    "# Split: last 12 months as test\n",
    "h = 12\n",
    "df_train = df[:-h].copy()\n",
    "df_test = df[-h:].copy()\n",
    "\n",
    "# Fit AutoARIMA on training set\n",
    "sf = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=12)],\n",
    "    freq='ME',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "sf.fit(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb4665c",
   "metadata": {},
   "source": [
    "Model is done in 3 seconds (on my machine). What RMSE are we getting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbdcd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Forecast the next 12 months\n",
    "df_forecast = sf.predict(h=h)\n",
    "df_forecast[\"y\"] = df_forecast[\"AutoARIMA\"]\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(df_test[\"y\"], df_forecast[\"y\"]))\n",
    "print(f\"RMSE on test set: {rmse:.4f} passengers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa632961",
   "metadata": {},
   "source": [
    "Let's look at the results in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f828523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine for plotting\n",
    "df_plot = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df_forecast['type'] = 'Forecast'\n",
    "df_test['type'] = 'Actual'\n",
    "\n",
    "# Plot actual vs forecast\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_plot['ds'], df_plot['y'], label='Train + Test', color='blue')\n",
    "plt.plot(df_forecast['ds'], df_forecast['y'], label='Forecast', color='orange')\n",
    "plt.axvline(df_test['ds'].iloc[0], linestyle='--', color='gray', label='Train/Test Split')\n",
    "plt.title('AutoARIMA Forecast vs Actual (StatsForecast)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Passengers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbd28b",
   "metadata": {},
   "source": [
    "## Continue with manual grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0404e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../files/AirPassengers.csv\")\n",
    "df['ds'] = pd.to_datetime(df['Month'])\n",
    "df.set_index('ds', inplace=True)\n",
    "y = df['#Passengers']\n",
    "\n",
    "# Train/test split: last 12 months as test\n",
    "train = y[:-12].copy()\n",
    "test = y[-12:].copy()\n",
    "\n",
    "# Define parameter ranges\n",
    "p = d = q = range(0, 3)\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in itertools.product(p, d, q)]\n",
    "\n",
    "# Grid search\n",
    "best_aic = np.inf\n",
    "best_order = None\n",
    "best_seasonal = None\n",
    "best_model = None\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "for order in itertools.product(p, d, q):\n",
    "    for seasonal_order in seasonal_pdq:\n",
    "        try:\n",
    "            model = SARIMAX(train,\n",
    "                            order=order,\n",
    "                            seasonal_order=seasonal_order,\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False)\n",
    "            results = model.fit(disp=False)\n",
    "\n",
    "            if results.aic < best_aic:\n",
    "                best_aic = results.aic\n",
    "                best_order = order\n",
    "                best_seasonal = seasonal_order\n",
    "                best_model = results\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "print(f\"Best model: ARIMA{best_order}x{best_seasonal} - AIC: {best_aic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e176a9d",
   "metadata": {},
   "source": [
    "This took a lot longer to train. That is because in stead of using statistical methods to determine the best value we're now simply trying them all. And \"all\" means 27 models. That still doesn't explain the 7 minutes this code-block took, though.\n",
    "\n",
    "But this line does:\n",
    "```Python\n",
    "for order in itertools.product(p, d, q):       # 27 combinations\n",
    "    for seasonal_order in seasonal_pdq:        # nested: 27 more\n",
    "```\n",
    "\n",
    "But why? We're tuning two different sets of parameters:\n",
    "* Non-seasonal order (p, d, q): Captures short-term autoregressive/moving average behavior\n",
    "* Seasonal order (P, D, Q, s): Captures repeating seasonal patterns (e.g., yearly seasonality for monthly data: s = 12)\n",
    "\n",
    "Each full SARIMA model needs both:\n",
    "\n",
    "```Python\n",
    "SARIMAX(train, order=(p,d,q), seasonal_order=(P,D,Q,s))\n",
    "```\n",
    "Se we're not training 27 models but 729. Auto_arima is faster because:\n",
    "* Early Stopping: If certain models are clearly worse, it prunes them quickly.\n",
    "* Caching + Optimization: auto_arima caches results and avoids refitting duplicates (e.g., if p=1, q=2 is the same as q=2, p=1 in effect).\n",
    "\n",
    "But what is the result we get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(test, forecast))\n",
    "print(f\"RMSE on test set: {rmse:.4f} passengers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f7a51",
   "metadata": {},
   "source": [
    "And the graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3115ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast the test period\n",
    "pred = best_model.get_forecast(steps=12)\n",
    "forecast = pred.predicted_mean\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train.index, train, label='Train')\n",
    "plt.plot(test.index, test, label='Test', color='gray')\n",
    "plt.plot(forecast.index, forecast, label='Forecast', color='orange')\n",
    "plt.axvline(test.index[0], linestyle='--', color='black', alpha=0.5)\n",
    "plt.title(f'Manual SARIMA Forecast (Best AIC: {best_aic:.2f})')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Passengers')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126ec6ae",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "| Feature          | `auto_arima()`          | Manual Grid Search    |\n",
    "| ---------------- | ----------------------- | --------------------- |\n",
    "| Easy to use      | Yes                     | Requires coding       |\n",
    "| Fast (stepwise)  | Yes                     |Slow if full grid      |\n",
    "| Seasonal support | Yes (`seasonal=True`)   |Yes (SARIMA)           |\n",
    "| Custom scoring   | Limited                 |Fully customizable     |\n",
    "| Best for         | Most use cases          | Research, fine-tuning |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
